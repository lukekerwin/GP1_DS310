{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Col 1</th>\n",
       "      <th>Col 2</th>\n",
       "      <th>Col 3</th>\n",
       "      <th>Col 4</th>\n",
       "      <th>Col 5</th>\n",
       "      <th>Col 6</th>\n",
       "      <th>Col 7</th>\n",
       "      <th>Col 8</th>\n",
       "      <th>Col 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Col 56</th>\n",
       "      <th>Col 57</th>\n",
       "      <th>Col 58</th>\n",
       "      <th>Col 59</th>\n",
       "      <th>Col 60</th>\n",
       "      <th>Col 61</th>\n",
       "      <th>Col 62</th>\n",
       "      <th>Col 63</th>\n",
       "      <th>Col 64</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022038</td>\n",
       "      <td>-0.031125</td>\n",
       "      <td>-0.000922</td>\n",
       "      <td>0.033494</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.031150</td>\n",
       "      <td>-0.028191</td>\n",
       "      <td>-0.017658</td>\n",
       "      <td>-0.027794</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011564</td>\n",
       "      <td>0.012973</td>\n",
       "      <td>0.023783</td>\n",
       "      <td>-0.023815</td>\n",
       "      <td>-0.094506</td>\n",
       "      <td>-0.140378</td>\n",
       "      <td>0.025298</td>\n",
       "      <td>0.053034</td>\n",
       "      <td>0.104013</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022063</td>\n",
       "      <td>-0.018016</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>0.032956</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>0.032795</td>\n",
       "      <td>-0.027332</td>\n",
       "      <td>-0.017236</td>\n",
       "      <td>-0.022304</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009984</td>\n",
       "      <td>-0.003373</td>\n",
       "      <td>-0.019109</td>\n",
       "      <td>0.008159</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>0.021514</td>\n",
       "      <td>-0.012045</td>\n",
       "      <td>-0.024872</td>\n",
       "      <td>-0.025042</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024091</td>\n",
       "      <td>-0.026846</td>\n",
       "      <td>-0.029687</td>\n",
       "      <td>0.030984</td>\n",
       "      <td>0.014489</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>-0.025575</td>\n",
       "      <td>-0.016180</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>155</td>\n",
       "      <td>-0.027310</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.060618</td>\n",
       "      <td>0.049415</td>\n",
       "      <td>0.085116</td>\n",
       "      <td>0.086368</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.037814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023104</td>\n",
       "      <td>0.056056</td>\n",
       "      <td>0.075937</td>\n",
       "      <td>0.029510</td>\n",
       "      <td>0.017894</td>\n",
       "      <td>0.010752</td>\n",
       "      <td>-0.001955</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>0.015606</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>156</td>\n",
       "      <td>-0.016412</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.010517</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.035760</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.021394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>0.022610</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>0.004828</td>\n",
       "      <td>-0.010755</td>\n",
       "      <td>0.008008</td>\n",
       "      <td>-0.006328</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>157</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.033151</td>\n",
       "      <td>-0.018294</td>\n",
       "      <td>0.031454</td>\n",
       "      <td>0.042840</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>0.019917</td>\n",
       "      <td>0.010226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010049</td>\n",
       "      <td>-0.006214</td>\n",
       "      <td>0.011544</td>\n",
       "      <td>0.026163</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>-0.023226</td>\n",
       "      <td>-0.007626</td>\n",
       "      <td>-0.015175</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>158</td>\n",
       "      <td>-0.012780</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.065486</td>\n",
       "      <td>-0.069938</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.016849</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>-0.007020</td>\n",
       "      <td>-0.030751</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025315</td>\n",
       "      <td>-0.027269</td>\n",
       "      <td>-0.032436</td>\n",
       "      <td>0.031763</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>0.017242</td>\n",
       "      <td>-0.022989</td>\n",
       "      <td>-0.011548</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>-0.005515</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.043373</td>\n",
       "      <td>0.087287</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.042345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023747</td>\n",
       "      <td>-0.009201</td>\n",
       "      <td>-0.016795</td>\n",
       "      <td>0.032060</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>0.019462</td>\n",
       "      <td>-0.029322</td>\n",
       "      <td>-0.017658</td>\n",
       "      <td>-0.035639</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     Col 1     Col 2     Col 3     Col 4     Col 5     Col 6  \\\n",
       "0      0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821   \n",
       "1      1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163   \n",
       "2      2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194   \n",
       "3      3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991   \n",
       "4      4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "155  155 -0.027310  0.050680  0.060618  0.049415  0.085116  0.086368   \n",
       "156  156 -0.016412 -0.044642 -0.010517  0.001215 -0.037344 -0.035760   \n",
       "157  157 -0.001882  0.050680 -0.033151 -0.018294  0.031454  0.042840   \n",
       "158  158 -0.012780 -0.044642 -0.065486 -0.069938  0.001183  0.016849   \n",
       "159  159 -0.005515 -0.044642  0.043373  0.087287  0.013567  0.007141   \n",
       "\n",
       "        Col 7     Col 8     Col 9  ...    Col 56    Col 57    Col 58  \\\n",
       "0   -0.043401 -0.002592  0.019908  ... -0.022038 -0.031125 -0.000922   \n",
       "1    0.074412 -0.039493 -0.068330  ... -0.011564  0.012973  0.023783   \n",
       "2   -0.032356 -0.002592  0.002864  ... -0.022063 -0.018016  0.004913   \n",
       "3   -0.036038  0.034309  0.022692  ... -0.009984 -0.003373 -0.019109   \n",
       "4    0.008142 -0.002592 -0.031991  ... -0.024091 -0.026846 -0.029687   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "155 -0.002903  0.034309  0.037814  ...  0.023104  0.056056  0.075937   \n",
       "156  0.011824 -0.039493 -0.021394  ... -0.001265  0.000986  0.012134   \n",
       "157 -0.013948  0.019917  0.010226  ... -0.010049 -0.006214  0.011544   \n",
       "158 -0.002903 -0.007020 -0.030751  ... -0.025315 -0.027269 -0.032436   \n",
       "159 -0.013948 -0.002592  0.042345  ... -0.023747 -0.009201 -0.016795   \n",
       "\n",
       "       Col 59    Col 60    Col 61    Col 62    Col 63    Col 64    y  \n",
       "0    0.033494  0.000852  0.031150 -0.028191 -0.017658 -0.027794  151  \n",
       "1   -0.023815 -0.094506 -0.140378  0.025298  0.053034  0.104013   75  \n",
       "2    0.032956  0.018281  0.032795 -0.027332 -0.017236 -0.022304  141  \n",
       "3    0.008159  0.001898  0.021514 -0.012045 -0.024872 -0.025042  206  \n",
       "4    0.030984  0.014489  0.005386 -0.025575 -0.016180  0.008735  135  \n",
       "..        ...       ...       ...       ...       ...       ...  ...  \n",
       "155  0.029510  0.017894  0.010752 -0.001955  0.014242  0.015606  186  \n",
       "156  0.022610  0.014659  0.004828 -0.010755  0.008008 -0.006328   25  \n",
       "157  0.026163  0.017152  0.005169 -0.023226 -0.007626 -0.015175   84  \n",
       "158  0.031763  0.022391  0.017242 -0.022989 -0.011548  0.010113   96  \n",
       "159  0.032060  0.007030  0.019462 -0.029322 -0.017658 -0.035639  195  \n",
       "\n",
       "[160 rows x 66 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = list(np.linspace(0, 100, 10))\n",
    "# alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# X = df.drop(columns=['id', 'y'])\n",
    "# y = df['y']\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# X = sc.fit_transform(X)\n",
    "\n",
    "# X_df = pd.DataFrame(X)\n",
    "\n",
    "# X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, random_state=23)\n",
    "\n",
    "# print(X_train.shape, X_val.shape)\n",
    "# y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import RidgeCV, Ridge\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# alpha_values = np.round(np.linspace(1, 100, 100), 0)\n",
    "# train_mse = []\n",
    "# val_mse = []\n",
    "# r2_scores = []\n",
    "\n",
    "# for alpha in alpha_values:\n",
    "#     ridge = Ridge(alpha=alpha)\n",
    "#     ridge.fit(X_train, y_train)\n",
    "    \n",
    "#     # Predict on the training and validation sets\n",
    "#     y_pred_train = ridge.predict(X_train)\n",
    "#     y_pred_val = ridge.predict(X_val)\n",
    "    \n",
    "#     # Calculate MSE and R^2 for training and validation sets\n",
    "#     train_mse.append(np.round(mean_squared_error(y_train, y_pred_train), 2))\n",
    "#     val_mse.append(np.round(mean_squared_error(y_val, y_pred_val), 2))\n",
    "#     r2_scores.append(np.round(r2_score(y_val, y_pred_val), 2))\n",
    "    \n",
    "#     print(f'For alpha {alpha}, the train MSE is {train_mse[-1]}, and the validation MSE is {val_mse[-1]}, R^2 is {r2_scores[-1]}')\n",
    "\n",
    "# # Find the alpha with the lowest validation MSE\n",
    "# best_alpha = alpha_values[np.argmin(val_mse)]\n",
    "# print(f'Best alpha: {best_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RidgeCV\n",
    "\n",
    "# ridge_cv = RidgeCV(alphas=alpha_values, cv=5)  # Create a RidgeCV model with cross-validation\n",
    "# ridge_cv.fit(X_train, y_train)  # Fit the model to your training data\n",
    "\n",
    "# # Access the best alpha and coefficients\n",
    "# best_alpha = ridge_cv.alpha_\n",
    "# coefficients = ridge_cv.coef_\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# y_pred_val = ridge_cv.predict(X_val)\n",
    "\n",
    "# # Calculate MSE and R^2 for validation set\n",
    "# val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "# r2 = r2_score(y_val, y_pred_val)\n",
    "\n",
    "# print(f'Best alpha: {best_alpha}')\n",
    "# print(f'Validation MSE: {val_mse}')\n",
    "# print(f'R^2 score on validation set: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ridge final model\n",
    "\n",
    "# model = Ridge(alpha = 50)\n",
    "# model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LassoCV\n",
    "\n",
    "# from sklearn.linear_model import LassoCV\n",
    "\n",
    "# lasso_cv = LassoCV(alphas=alpha_values, cv=5)  # Create a LassoCV model with cross-validation\n",
    "# lasso_cv.fit(X_train, y_train)  # Fit the model to your training data\n",
    "\n",
    "# # Access the best alpha and coefficients\n",
    "# best_alpha = lasso_cv.alpha_\n",
    "# coefficients = lasso_cv.coef_\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# y_pred_val = lasso_cv.predict(X_val)\n",
    "\n",
    "# # Calculate MSE and R^2 for validation set\n",
    "# val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "# r2 = r2_score(y_val, y_pred_val)\n",
    "\n",
    "# print(f'Best alpha: {best_alpha}')\n",
    "# print(f'Validation MSE: {val_mse}')\n",
    "# print(f'R^2 score on validation set: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lasso final model\n",
    "# from sklearn.linear_model import Lasso\n",
    "\n",
    "# model = Lasso(alpha = 3)\n",
    "# model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160 entries, 0 to 159\n",
      "Data columns (total 66 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      160 non-null    int64  \n",
      " 1   Col 1   160 non-null    float64\n",
      " 2   Col 2   160 non-null    float64\n",
      " 3   Col 3   160 non-null    float64\n",
      " 4   Col 4   160 non-null    float64\n",
      " 5   Col 5   160 non-null    float64\n",
      " 6   Col 6   160 non-null    float64\n",
      " 7   Col 7   160 non-null    float64\n",
      " 8   Col 8   160 non-null    float64\n",
      " 9   Col 9   160 non-null    float64\n",
      " 10  Col 10  160 non-null    float64\n",
      " 11  Col 11  160 non-null    float64\n",
      " 12  Col 12  160 non-null    float64\n",
      " 13  Col 13  160 non-null    float64\n",
      " 14  Col 14  160 non-null    float64\n",
      " 15  Col 15  160 non-null    float64\n",
      " 16  Col 16  160 non-null    float64\n",
      " 17  Col 17  160 non-null    float64\n",
      " 18  Col 18  160 non-null    float64\n",
      " 19  Col 19  160 non-null    float64\n",
      " 20  Col 20  160 non-null    float64\n",
      " 21  Col 21  160 non-null    float64\n",
      " 22  Col 22  160 non-null    float64\n",
      " 23  Col 23  160 non-null    float64\n",
      " 24  Col 24  160 non-null    float64\n",
      " 25  Col 25  160 non-null    float64\n",
      " 26  Col 26  160 non-null    float64\n",
      " 27  Col 27  160 non-null    float64\n",
      " 28  Col 28  160 non-null    float64\n",
      " 29  Col 29  160 non-null    float64\n",
      " 30  Col 30  160 non-null    float64\n",
      " 31  Col 31  160 non-null    float64\n",
      " 32  Col 32  160 non-null    float64\n",
      " 33  Col 33  160 non-null    float64\n",
      " 34  Col 34  160 non-null    float64\n",
      " 35  Col 35  160 non-null    float64\n",
      " 36  Col 36  160 non-null    float64\n",
      " 37  Col 37  160 non-null    float64\n",
      " 38  Col 38  160 non-null    float64\n",
      " 39  Col 39  160 non-null    float64\n",
      " 40  Col 40  160 non-null    float64\n",
      " 41  Col 41  160 non-null    float64\n",
      " 42  Col 42  160 non-null    float64\n",
      " 43  Col 43  160 non-null    float64\n",
      " 44  Col 44  160 non-null    float64\n",
      " 45  Col 45  160 non-null    float64\n",
      " 46  Col 46  160 non-null    float64\n",
      " 47  Col 47  160 non-null    float64\n",
      " 48  Col 48  160 non-null    float64\n",
      " 49  Col 49  160 non-null    float64\n",
      " 50  Col 50  160 non-null    float64\n",
      " 51  Col 51  160 non-null    float64\n",
      " 52  Col 52  160 non-null    float64\n",
      " 53  Col 53  160 non-null    float64\n",
      " 54  Col 54  160 non-null    float64\n",
      " 55  Col 55  160 non-null    float64\n",
      " 56  Col 56  160 non-null    float64\n",
      " 57  Col 57  160 non-null    float64\n",
      " 58  Col 58  160 non-null    float64\n",
      " 59  Col 59  160 non-null    float64\n",
      " 60  Col 60  160 non-null    float64\n",
      " 61  Col 61  160 non-null    float64\n",
      " 62  Col 62  160 non-null    float64\n",
      " 63  Col 63  160 non-null    float64\n",
      " 64  Col 64  160 non-null    float64\n",
      " 65  y       160 non-null    int64  \n",
      "dtypes: float64(64), int64(2)\n",
      "memory usage: 82.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20      68\n",
       "144    174\n",
       "61     144\n",
       "88      42\n",
       "139    281\n",
       "      ... \n",
       "39      90\n",
       "91     164\n",
       "31      59\n",
       "40     100\n",
       "83     210\n",
       "Name: y, Length: 128, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "Col 1     0\n",
       "Col 2     0\n",
       "Col 3     0\n",
       "Col 4     0\n",
       "         ..\n",
       "Col 61    0\n",
       "Col 62    0\n",
       "Col 63    0\n",
       "Col 64    0\n",
       "y         0\n",
       "Length: 66, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      151\n",
       "1       75\n",
       "2      141\n",
       "3      206\n",
       "4      135\n",
       "      ... \n",
       "155    186\n",
       "156     25\n",
       "157     84\n",
       "158     96\n",
       "159    195\n",
       "Name: y, Length: 160, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([15., 26., 29., 24., 21., 14.,  6., 12.,  9.,  4.]),\n",
       " array([ 25. ,  56.6,  88.2, 119.8, 151.4, 183. , 214.6, 246.2, 277.8,\n",
       "        309.4, 341. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Col 1</th>\n",
       "      <th>Col 2</th>\n",
       "      <th>Col 3</th>\n",
       "      <th>Col 4</th>\n",
       "      <th>Col 5</th>\n",
       "      <th>Col 6</th>\n",
       "      <th>Col 7</th>\n",
       "      <th>Col 8</th>\n",
       "      <th>Col 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Col 56</th>\n",
       "      <th>Col 57</th>\n",
       "      <th>Col 58</th>\n",
       "      <th>Col 59</th>\n",
       "      <th>Col 60</th>\n",
       "      <th>Col 61</th>\n",
       "      <th>Col 62</th>\n",
       "      <th>Col 63</th>\n",
       "      <th>Col 64</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>79.500000</td>\n",
       "      <td>-0.008875</td>\n",
       "      <td>-0.002343</td>\n",
       "      <td>-0.001120</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>-0.006032</td>\n",
       "      <td>-0.004778</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>-0.005934</td>\n",
       "      <td>-0.006234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>-0.005031</td>\n",
       "      <td>-0.000871</td>\n",
       "      <td>-0.001619</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>-0.002643</td>\n",
       "      <td>-0.005465</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>-0.001809</td>\n",
       "      <td>147.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>46.332134</td>\n",
       "      <td>0.048101</td>\n",
       "      <td>0.047507</td>\n",
       "      <td>0.046042</td>\n",
       "      <td>0.045894</td>\n",
       "      <td>0.043050</td>\n",
       "      <td>0.045374</td>\n",
       "      <td>0.048762</td>\n",
       "      <td>0.047204</td>\n",
       "      <td>0.044045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055793</td>\n",
       "      <td>0.043475</td>\n",
       "      <td>0.049456</td>\n",
       "      <td>0.047331</td>\n",
       "      <td>0.045093</td>\n",
       "      <td>0.056129</td>\n",
       "      <td>0.039382</td>\n",
       "      <td>0.053606</td>\n",
       "      <td>0.047778</td>\n",
       "      <td>76.364695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.107226</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.083808</td>\n",
       "      <td>-0.108957</td>\n",
       "      <td>-0.126781</td>\n",
       "      <td>-0.106845</td>\n",
       "      <td>-0.102307</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>-0.126097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076433</td>\n",
       "      <td>-0.155145</td>\n",
       "      <td>-0.151734</td>\n",
       "      <td>-0.228580</td>\n",
       "      <td>-0.143972</td>\n",
       "      <td>-0.223255</td>\n",
       "      <td>-0.160745</td>\n",
       "      <td>-0.128919</td>\n",
       "      <td>-0.092165</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.750000</td>\n",
       "      <td>-0.046381</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.035307</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.034273</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.034524</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023703</td>\n",
       "      <td>-0.022007</td>\n",
       "      <td>-0.020446</td>\n",
       "      <td>-0.021084</td>\n",
       "      <td>-0.018638</td>\n",
       "      <td>-0.017585</td>\n",
       "      <td>-0.026875</td>\n",
       "      <td>-0.019626</td>\n",
       "      <td>-0.023964</td>\n",
       "      <td>87.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>79.500000</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.006206</td>\n",
       "      <td>-0.012556</td>\n",
       "      <td>-0.005009</td>\n",
       "      <td>-0.009925</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014868</td>\n",
       "      <td>-0.009495</td>\n",
       "      <td>-0.010498</td>\n",
       "      <td>0.014418</td>\n",
       "      <td>0.009642</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>-0.014899</td>\n",
       "      <td>-0.015336</td>\n",
       "      <td>-0.015265</td>\n",
       "      <td>134.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>119.250000</td>\n",
       "      <td>0.030811</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.028284</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.020446</td>\n",
       "      <td>0.021703</td>\n",
       "      <td>0.037595</td>\n",
       "      <td>0.025822</td>\n",
       "      <td>0.026221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003447</td>\n",
       "      <td>0.013173</td>\n",
       "      <td>0.009096</td>\n",
       "      <td>0.031522</td>\n",
       "      <td>0.022619</td>\n",
       "      <td>0.022176</td>\n",
       "      <td>0.014464</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.011668</td>\n",
       "      <td>195.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>159.000000</td>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.128521</td>\n",
       "      <td>0.125158</td>\n",
       "      <td>0.152538</td>\n",
       "      <td>0.198788</td>\n",
       "      <td>0.181179</td>\n",
       "      <td>0.185234</td>\n",
       "      <td>0.133599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555129</td>\n",
       "      <td>0.113164</td>\n",
       "      <td>0.299032</td>\n",
       "      <td>0.080445</td>\n",
       "      <td>0.163067</td>\n",
       "      <td>0.209905</td>\n",
       "      <td>0.157844</td>\n",
       "      <td>0.318104</td>\n",
       "      <td>0.338184</td>\n",
       "      <td>341.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       Col 1       Col 2       Col 3       Col 4       Col 5  \\\n",
       "count  160.000000  160.000000  160.000000  160.000000  160.000000  160.000000   \n",
       "mean    79.500000   -0.008875   -0.002343   -0.001120   -0.004401   -0.006032   \n",
       "std     46.332134    0.048101    0.047507    0.046042    0.045894    0.043050   \n",
       "min      0.000000   -0.107226   -0.044642   -0.083808   -0.108957   -0.126781   \n",
       "25%     39.750000   -0.046381   -0.044642   -0.035307   -0.036656   -0.037344   \n",
       "50%     79.500000   -0.001882   -0.044642   -0.006206   -0.012556   -0.005009   \n",
       "75%    119.250000    0.030811    0.050680    0.028284    0.021872    0.020446   \n",
       "max    159.000000    0.085299    0.050680    0.128521    0.125158    0.152538   \n",
       "\n",
       "            Col 6       Col 7       Col 8       Col 9  ...      Col 56  \\\n",
       "count  160.000000  160.000000  160.000000  160.000000  ...  160.000000   \n",
       "mean    -0.004778    0.002827   -0.005934   -0.006234  ...   -0.000659   \n",
       "std      0.045374    0.048762    0.047204    0.044045  ...    0.055793   \n",
       "min     -0.106845   -0.102307   -0.076395   -0.126097  ...   -0.076433   \n",
       "25%     -0.034273   -0.032356   -0.039493   -0.034524  ...   -0.023703   \n",
       "50%     -0.009925   -0.002903   -0.002592   -0.010412  ...   -0.014868   \n",
       "75%      0.021703    0.037595    0.025822    0.026221  ...    0.003447   \n",
       "max      0.198788    0.181179    0.185234    0.133599  ...    0.555129   \n",
       "\n",
       "           Col 57      Col 58      Col 59      Col 60      Col 61      Col 62  \\\n",
       "count  160.000000  160.000000  160.000000  160.000000  160.000000  160.000000   \n",
       "mean    -0.005031   -0.000871   -0.001619    0.002133   -0.002643   -0.005465   \n",
       "std      0.043475    0.049456    0.047331    0.045093    0.056129    0.039382   \n",
       "min     -0.155145   -0.151734   -0.228580   -0.143972   -0.223255   -0.160745   \n",
       "25%     -0.022007   -0.020446   -0.021084   -0.018638   -0.017585   -0.026875   \n",
       "50%     -0.009495   -0.010498    0.014418    0.009642    0.009252   -0.014899   \n",
       "75%      0.013173    0.009096    0.031522    0.022619    0.022176    0.014464   \n",
       "max      0.113164    0.299032    0.080445    0.163067    0.209905    0.157844   \n",
       "\n",
       "           Col 63      Col 64           y  \n",
       "count  160.000000  160.000000  160.000000  \n",
       "mean     0.001668   -0.001809  147.843750  \n",
       "std      0.053606    0.047778   76.364695  \n",
       "min     -0.128919   -0.092165   25.000000  \n",
       "25%     -0.019626   -0.023964   87.750000  \n",
       "50%     -0.015336   -0.015265  134.500000  \n",
       "75%      0.018200    0.011668  195.500000  \n",
       "max      0.318104    0.338184  341.000000  \n",
       "\n",
       "[8 rows x 66 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop(columns=['y'])\n",
    "# y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=23)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_reg = LinearRegression()\n",
    "# lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# np.round(mse, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find best principal components by mse score\n",
    "\n",
    "# # Intialize mse\n",
    "# mse = []\n",
    "# num_comps = []\n",
    "\n",
    "# for comp in range(1, len(X.columns)):\n",
    "#     pca = PCA(n_components=comp)\n",
    "#     X_pca = pca.fit_transform(X)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=23)\n",
    "\n",
    "#     lr = LinearRegression()\n",
    "#     lr.fit(X_train, y_train)\n",
    "\n",
    "#     y_pred = lr.predict(X_test)\n",
    "#     mse.append(mean_squared_error(y_test, y_pred))\n",
    "#     num_comps.append(comp)\n",
    "\n",
    "# plt.scatter(num_comps, mse)\n",
    "# plt.plot(num_comps, mse)\n",
    "# plt.title(\"MSE by Principal Components\")\n",
    "# plt.xlabel(\"Number of Principal Components\")\n",
    "# plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_mse = min(mse)\n",
    "# best_num_comp = np.argmin(mse)+1\n",
    "\n",
    "# print(f'Best number of principal components: {best_num_comp}\\nBest MSE: {best_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# X = df.drop(columns=(['id', 'y']))\n",
    "# y = df['y']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LinearRegression()\n",
    "# lr.fit(X_train, y_train)\n",
    "# y_pred = lr.predict(X_test)\n",
    "\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(mse)\n",
    "# print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mse of 5702 is baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# # Create a heatmap\n",
    "# plt.figure(figsize=(40, 12))\n",
    "# sns.heatmap(X.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'rf__max_depth': None, 'rf__min_samples_leaf': 4, 'rf__min_samples_split': 10, 'rf__n_estimators': 200}\n",
      "Mean Squared Error: 3601.807806699485\n",
      "0.20082408448580336\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a KFold object for cross-validation\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=23)\n",
    "\n",
    "# Create a RandomForestRegressor instance\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('rf', rf)\n",
    "])\n",
    "\n",
    "# Create GridSearchCV with the pipeline\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=k_fold, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding mean squared error\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(min_samples_leaf=4, min_samples_split=10,\n",
       "                      n_estimators=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(min_samples_leaf=4, min_samples_split=10,\n",
       "                      n_estimators=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(min_samples_leaf=4, min_samples_split=10,\n",
       "                      n_estimators=200)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final fit\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=best_params['rf__n_estimators'],  # Replace with your best hyperparameters\n",
    "    max_depth=best_params['rf__max_depth'],\n",
    "    min_samples_split=best_params['rf__min_samples_split'],\n",
    "    min_samples_leaf=best_params['rf__min_samples_leaf']\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)  # Replace with your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 3487.6648919007657\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)  # Replace with your testing data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on Test Data:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 4429.25551264716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(\n",
    "    n_estimators=100,             # Number of boosting rounds (trees)\n",
    "    max_depth=3,                  # Maximum tree depth\n",
    "    learning_rate=0.1,            # Learning rate\n",
    "    random_state=42               # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model to your training data\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance on the test data (optional)\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on Test Data:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.005, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "MSE: 3996.6755068068624\n",
      "R-squared: 0.11321009376888336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.005, 0.01, 0.1],\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'min_samples_split': [5, 10, 15],\n",
    "}\n",
    "\n",
    "# Perfrom grid search\n",
    "\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingRegressor(),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error',  # Use negative mean squared error for regression\n",
    "                           cv=5,  # Cross-validation folds\n",
    "                           n_jobs=-1)  # Use all available CPU cores\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Able to account for around 33% of explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=20)\n",
    "# X_pca = pca.fit_transform(X)\n",
    "\n",
    "# X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a DataFrame for the principal components\n",
    "# pca_df = pd.DataFrame(data=X_pca, columns=[f'PC{i+1}' for i in range(20)])\n",
    "\n",
    "# # Concatenate the PCA DataFrame with your original DataFrame X\n",
    "# X_with_pca = pd.concat([X, pca_df], axis=1)\n",
    "\n",
    "# X_with_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=(['id', 'y']))\n",
    "y = df['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "MSE: 3136.7987170652177\n",
      "R-squared: 0.33463753350812475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'learning_rate': [0.005, 0.01, 0.1],\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'min_samples_split': [5, 10, 15],\n",
    "}\n",
    "\n",
    "# Perfrom grid search\n",
    "\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingRegressor(),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error',  # Use negative mean squared error for regression\n",
    "                           cv=5,  # Cross-validation folds\n",
    "                           n_jobs=-1)  # Use all available CPU cores\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBR Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature  Importance\n",
      "8    Col 9    0.326370\n",
      "2    Col 3    0.274193\n",
      "36  Col 37    0.046811\n",
      "3    Col 4    0.043444\n",
      "55  Col 56    0.041974\n",
      "..     ...         ...\n",
      "26  Col 27    0.000000\n",
      "13  Col 14    0.000000\n",
      "20  Col 21    0.000000\n",
      "17  Col 18    0.000000\n",
      "0    Col 1    0.000000\n",
      "\n",
      "[64 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# # Look into feature importances\n",
    "\n",
    "# # Access feature importances\n",
    "# feature_importances = model.feature_importances_\n",
    "\n",
    "# # Create a DataFrame to display feature names and their importances\n",
    "# feature_names = X.columns  # Assuming X is your feature matrix\n",
    "# feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "# # Sort the features by importance in descending order\n",
    "# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Print or visualize the feature importance information\n",
    "# print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that many features have 0 feature importance based upon our gradientboostingregressor feature importance output. Let's remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col 2</th>\n",
       "      <th>Col 3</th>\n",
       "      <th>Col 4</th>\n",
       "      <th>Col 5</th>\n",
       "      <th>Col 6</th>\n",
       "      <th>Col 7</th>\n",
       "      <th>Col 8</th>\n",
       "      <th>Col 9</th>\n",
       "      <th>Col 10</th>\n",
       "      <th>Col 12</th>\n",
       "      <th>...</th>\n",
       "      <th>Col 52</th>\n",
       "      <th>Col 53</th>\n",
       "      <th>Col 56</th>\n",
       "      <th>Col 57</th>\n",
       "      <th>Col 58</th>\n",
       "      <th>Col 59</th>\n",
       "      <th>Col 60</th>\n",
       "      <th>Col 61</th>\n",
       "      <th>Col 63</th>\n",
       "      <th>Col 64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>0.022505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019303</td>\n",
       "      <td>-0.042892</td>\n",
       "      <td>-0.022038</td>\n",
       "      <td>-0.031125</td>\n",
       "      <td>-0.000922</td>\n",
       "      <td>0.033494</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.031150</td>\n",
       "      <td>-0.017658</td>\n",
       "      <td>-0.027794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>0.005643</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015501</td>\n",
       "      <td>-0.012343</td>\n",
       "      <td>-0.011564</td>\n",
       "      <td>0.012973</td>\n",
       "      <td>0.023783</td>\n",
       "      <td>-0.023815</td>\n",
       "      <td>-0.094506</td>\n",
       "      <td>-0.140378</td>\n",
       "      <td>0.053034</td>\n",
       "      <td>0.104013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>-0.004176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019241</td>\n",
       "      <td>-0.027178</td>\n",
       "      <td>-0.022063</td>\n",
       "      <td>-0.018016</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>0.032956</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>0.032795</td>\n",
       "      <td>-0.017236</td>\n",
       "      <td>-0.022304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>-0.031017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014033</td>\n",
       "      <td>-0.018644</td>\n",
       "      <td>-0.009984</td>\n",
       "      <td>-0.003373</td>\n",
       "      <td>-0.019109</td>\n",
       "      <td>0.008159</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>0.021514</td>\n",
       "      <td>-0.024872</td>\n",
       "      <td>-0.025042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>-0.013681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021470</td>\n",
       "      <td>-0.027079</td>\n",
       "      <td>-0.024091</td>\n",
       "      <td>-0.026846</td>\n",
       "      <td>-0.029687</td>\n",
       "      <td>0.030984</td>\n",
       "      <td>0.014489</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>-0.016180</td>\n",
       "      <td>0.008735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.060618</td>\n",
       "      <td>0.049415</td>\n",
       "      <td>0.085116</td>\n",
       "      <td>0.086368</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.037814</td>\n",
       "      <td>0.048628</td>\n",
       "      <td>0.020583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029396</td>\n",
       "      <td>0.043009</td>\n",
       "      <td>0.023104</td>\n",
       "      <td>0.056056</td>\n",
       "      <td>0.075937</td>\n",
       "      <td>0.029510</td>\n",
       "      <td>0.017894</td>\n",
       "      <td>0.010752</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>0.015606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.010517</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.035760</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.021394</td>\n",
       "      <td>-0.034215</td>\n",
       "      <td>-0.031364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>-0.007699</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>0.022610</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>0.004828</td>\n",
       "      <td>0.008008</td>\n",
       "      <td>-0.006328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.033151</td>\n",
       "      <td>-0.018294</td>\n",
       "      <td>0.031454</td>\n",
       "      <td>0.042840</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>0.019917</td>\n",
       "      <td>0.010226</td>\n",
       "      <td>0.027917</td>\n",
       "      <td>-0.016958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010419</td>\n",
       "      <td>-0.017701</td>\n",
       "      <td>-0.010049</td>\n",
       "      <td>-0.006214</td>\n",
       "      <td>0.011544</td>\n",
       "      <td>0.026163</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>-0.007626</td>\n",
       "      <td>-0.015175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.065486</td>\n",
       "      <td>-0.069938</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.016849</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>-0.007020</td>\n",
       "      <td>-0.030751</td>\n",
       "      <td>-0.050783</td>\n",
       "      <td>0.029529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021437</td>\n",
       "      <td>-0.025203</td>\n",
       "      <td>-0.025315</td>\n",
       "      <td>-0.027269</td>\n",
       "      <td>-0.032436</td>\n",
       "      <td>0.031763</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>0.017242</td>\n",
       "      <td>-0.011548</td>\n",
       "      <td>0.010113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.043373</td>\n",
       "      <td>0.087287</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.042345</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>-0.005556</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021903</td>\n",
       "      <td>-0.012402</td>\n",
       "      <td>-0.023747</td>\n",
       "      <td>-0.009201</td>\n",
       "      <td>-0.016795</td>\n",
       "      <td>0.032060</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>0.019462</td>\n",
       "      <td>-0.017658</td>\n",
       "      <td>-0.035639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Col 2     Col 3     Col 4     Col 5     Col 6     Col 7     Col 8  \\\n",
       "0    0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401 -0.002592   \n",
       "1   -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412 -0.039493   \n",
       "2    0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356 -0.002592   \n",
       "3   -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038  0.034309   \n",
       "4   -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142 -0.002592   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "155  0.050680  0.060618  0.049415  0.085116  0.086368 -0.002903  0.034309   \n",
       "156 -0.044642 -0.010517  0.001215 -0.037344 -0.035760  0.011824 -0.039493   \n",
       "157  0.050680 -0.033151 -0.018294  0.031454  0.042840 -0.013948  0.019917   \n",
       "158 -0.044642 -0.065486 -0.069938  0.001183  0.016849 -0.002903 -0.007020   \n",
       "159 -0.044642  0.043373  0.087287  0.013567  0.007141 -0.013948 -0.002592   \n",
       "\n",
       "        Col 9    Col 10    Col 12  ...    Col 52    Col 53    Col 56  \\\n",
       "0    0.019908 -0.017646  0.022505  ... -0.019303 -0.042892 -0.022038   \n",
       "1   -0.068330 -0.092204  0.005643  ... -0.015501 -0.012343 -0.011564   \n",
       "2    0.002864 -0.025930 -0.004176  ... -0.019241 -0.027178 -0.022063   \n",
       "3    0.022692 -0.009362 -0.031017  ... -0.014033 -0.018644 -0.009984   \n",
       "4   -0.031991 -0.046641 -0.013681  ... -0.021470 -0.027079 -0.024091   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "155  0.037814  0.048628  0.020583  ...  0.029396  0.043009  0.023104   \n",
       "156 -0.021394 -0.034215 -0.031364  ...  0.004307 -0.007699 -0.001265   \n",
       "157  0.010226  0.027917 -0.016958  ... -0.010419 -0.017701 -0.010049   \n",
       "158 -0.030751 -0.050783  0.029529  ... -0.021437 -0.025203 -0.025315   \n",
       "159  0.042345 -0.017646 -0.005556  ... -0.021903 -0.012402 -0.023747   \n",
       "\n",
       "       Col 57    Col 58    Col 59    Col 60    Col 61    Col 63    Col 64  \n",
       "0   -0.031125 -0.000922  0.033494  0.000852  0.031150 -0.017658 -0.027794  \n",
       "1    0.012973  0.023783 -0.023815 -0.094506 -0.140378  0.053034  0.104013  \n",
       "2   -0.018016  0.004913  0.032956  0.018281  0.032795 -0.017236 -0.022304  \n",
       "3   -0.003373 -0.019109  0.008159  0.001898  0.021514 -0.024872 -0.025042  \n",
       "4   -0.026846 -0.029687  0.030984  0.014489  0.005386 -0.016180  0.008735  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "155  0.056056  0.075937  0.029510  0.017894  0.010752  0.014242  0.015606  \n",
       "156  0.000986  0.012134  0.022610  0.014659  0.004828  0.008008 -0.006328  \n",
       "157 -0.006214  0.011544  0.026163  0.017152  0.005169 -0.007626 -0.015175  \n",
       "158 -0.027269 -0.032436  0.031763  0.022391  0.017242 -0.011548  0.010113  \n",
       "159 -0.009201 -0.016795  0.032060  0.007030  0.019462 -0.017658 -0.035639  \n",
       "\n",
       "[160 rows x 50 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create a mask to identify features with non-zero importance\n",
    "# non_zero_importance_mask = feature_importances != 0\n",
    "\n",
    "# # Use the mask to select the relevant features from your feature matrix X\n",
    "# X_reduced = X.loc[:, non_zero_importance_mask]\n",
    "\n",
    "# X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "MSE: 3573.7931682405433\n",
      "R-squared: 0.40821890935367766\n"
     ]
    }
   ],
   "source": [
    "# # Train new model\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [200, 300, 400],\n",
    "#     'learning_rate': [0.005, 0.01, 0.1],\n",
    "#     'max_depth': [1, 2, 3],\n",
    "#     'min_samples_split': [5, 10, 15],\n",
    "# }\n",
    "\n",
    "# # Perfrom grid search\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=GradientBoostingRegressor(),\n",
    "#                            param_grid=param_grid,\n",
    "#                            scoring='neg_mean_squared_error',  # Use negative mean squared error for regression\n",
    "#                            cv=5,  # Cross-validation folds\n",
    "#                            n_jobs=-1)  # Use all available CPU cores\n",
    "\n",
    "\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_params = grid_search.best_params_\n",
    "# model = grid_search.best_estimator_\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(f'Best Parameters: {best_params}')\n",
    "# print(f'MSE: {mse}')\n",
    "# print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df['id'], df['y'])\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('y Values')\n",
    "plt.title('Scatter Plot of y Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# For speed\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Set a random seed for NumPy and scikit-learn for reproducibility\n",
    "# np.random.seed(42)\n",
    "# random_state = 42\n",
    "\n",
    "# Initialize the PolynomialFeatures transformer\n",
    "poly_features = PolynomialFeatures(degree=2)  # You can choose the desired degree\n",
    "\n",
    "# Preprocess the training data with polynomial features\n",
    "X_train_poly = poly_features.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 400, 'min_samples_split': 15, 'max_depth': 3, 'learning_rate': 0.005}\n",
      "MSE: 2471.642519821844\n",
      "R-squared: 0.4757272265102549\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(200, 501, 100),  # Randomly sample from this range\n",
    "    'learning_rate': [0.005, 0.01, 0.1],\n",
    "    'max_depth': np.arange(1, 6),  # Randomly sample from this range\n",
    "    'min_samples_split': np.arange(5, 16),  # Randomly sample from this range\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=GradientBoostingRegressor(),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,  # Cross-validation folds\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    "    # random_state = random_state  # Set a random seed\n",
    ")\n",
    "\n",
    "# Fit the random search on the preprocessed training data\n",
    "random_search.fit(X_train_poly, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "model = random_search.best_estimator_\n",
    "\n",
    "# Preprocess the test data with polynomial features based on the selected degree\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) and R-squared (R2) score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 400, 'min_samples_split': 15, 'max_depth': 3, 'learning_rate': 0.005}\n",
      "MSE: 2469.459276928491\n",
      "R-squared: 0.47619032535959116\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=400, min_samples_split=15, max_depth=3, learning_rate=0.005)\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Preprocess the test data with polynomial features based on the selected degree\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) and R-squared (R2) score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model produces MSE of 2469 and R-squared of 0.476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 1, 'learning_rate': 0.005}\n",
      "MSE: 2893.855580481346\n",
      "R-squared: 0.38616944841720513\n"
     ]
    }
   ],
   "source": [
    "# model = GradientBoostingRegressor(n_estimators=500, min_samples_split=12, max_depth=1, learning_rate=0.005)\n",
    "# model.fit(X_train_poly, y_train)\n",
    "\n",
    "# # Preprocess the test data with polynomial features based on the selected degree\n",
    "# X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "# # Make predictions with the best model\n",
    "# y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# # Calculate Mean Squared Error (MSE) and R-squared (R2) score\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"MSE:\", mse)\n",
    "# print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model produces MSE of 2893 and R-squared of 0.386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bdogellis/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 8 is smaller than n_iter=50. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 500, 'min_samples_split': 12, 'max_depth': 1, 'learning_rate': 0.005}\n",
      "MSE: 2893.8555804813464\n",
      "R-squared: 0.386169448417205\n"
     ]
    }
   ],
   "source": [
    "# # Define the parameter grid for RandomizedSearchCV\n",
    "# param_dist = {\n",
    "#     'n_estimators': [500, 600],  # Randomly sample from this range\n",
    "#     'learning_rate': [0.001, 0.005],\n",
    "#     'max_depth': [1, 2],  # Randomly sample from this range\n",
    "#     'min_samples_split': [12],  # Randomly sample from this range\n",
    "# }\n",
    "\n",
    "# # Initialize RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=GradientBoostingRegressor(),\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=50,  # Number of random combinations to try\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     cv=5,  # Cross-validation folds\n",
    "#     n_jobs=-1  # Use all available CPU cores\n",
    "#     # random_state = random_state  # Set a random seed\n",
    "# )\n",
    "\n",
    "# # Fit the random search on the preprocessed training data\n",
    "# random_search.fit(X_train_poly, y_train)\n",
    "\n",
    "# best_params = random_search.best_params_\n",
    "# model = random_search.best_estimator_\n",
    "\n",
    "# # Preprocess the test data with polynomial features based on the selected degree\n",
    "# X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "# # Make predictions with the best model\n",
    "# y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# # Calculate Mean Squared Error (MSE) and R-squared (R2) score\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"MSE:\", mse)\n",
    "# print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same results for the above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.005, min_samples_split=15,\n",
       "                          n_estimators=400)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.005, min_samples_split=15,\n",
       "                          n_estimators=400)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.005, min_samples_split=15,\n",
       "                          n_estimators=400)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_regressor = SVR(kernel='linear', C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_regressor,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error',  # Use negative MSE for regression\n",
    "                           cv=5,  # Cross-validation folds\n",
    "                           n_jobs=-1)  # Use all available CPU cores\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_svm_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "MSE: 4415.380779806152\n",
      "R-squared: 0.06343093352790352\n"
     ]
    }
   ],
   "source": [
    "best_svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=2, min_samples_split=5,\n",
       "                          n_estimators=300)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=2, min_samples_split=5,\n",
       "                          n_estimators=300)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.01, max_depth=2, min_samples_split=5,\n",
       "                          n_estimators=300)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "\n",
    "x_test_kaggle = pd.read_csv('x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop ID\n",
    "\n",
    "x_test_kaggle_drop = x_test_kaggle.drop('id', axis=1)\n",
    "\n",
    "# Get predictions \n",
    "\n",
    "y_pred_kaggle = model.predict(x_test_kaggle_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df = pd.read_csv('sample_submission.csv')\n",
    "kaggle_df['y'] = y_pred_kaggle\n",
    "\n",
    "\n",
    "# kaggle_df.to_csv('submission.csv', index=False)\n",
    "kaggle_df = kaggle_df[['id', 'y']]\n",
    "kaggle_df.to_csv('brian_submission_GBR_RandomGridSearchCV.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cde9e06954608812f36a56132da0251c351f6ad8984d203ba87e4f78021e1e3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
