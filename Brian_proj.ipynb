{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Col 1</th>\n",
       "      <th>Col 2</th>\n",
       "      <th>Col 3</th>\n",
       "      <th>Col 4</th>\n",
       "      <th>Col 5</th>\n",
       "      <th>Col 6</th>\n",
       "      <th>Col 7</th>\n",
       "      <th>Col 8</th>\n",
       "      <th>Col 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Col 56</th>\n",
       "      <th>Col 57</th>\n",
       "      <th>Col 58</th>\n",
       "      <th>Col 59</th>\n",
       "      <th>Col 60</th>\n",
       "      <th>Col 61</th>\n",
       "      <th>Col 62</th>\n",
       "      <th>Col 63</th>\n",
       "      <th>Col 64</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022038</td>\n",
       "      <td>-0.031125</td>\n",
       "      <td>-0.000922</td>\n",
       "      <td>0.033494</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.031150</td>\n",
       "      <td>-0.028191</td>\n",
       "      <td>-0.017658</td>\n",
       "      <td>-0.027794</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011564</td>\n",
       "      <td>0.012973</td>\n",
       "      <td>0.023783</td>\n",
       "      <td>-0.023815</td>\n",
       "      <td>-0.094506</td>\n",
       "      <td>-0.140378</td>\n",
       "      <td>0.025298</td>\n",
       "      <td>0.053034</td>\n",
       "      <td>0.104013</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022063</td>\n",
       "      <td>-0.018016</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>0.032956</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>0.032795</td>\n",
       "      <td>-0.027332</td>\n",
       "      <td>-0.017236</td>\n",
       "      <td>-0.022304</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009984</td>\n",
       "      <td>-0.003373</td>\n",
       "      <td>-0.019109</td>\n",
       "      <td>0.008159</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>0.021514</td>\n",
       "      <td>-0.012045</td>\n",
       "      <td>-0.024872</td>\n",
       "      <td>-0.025042</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024091</td>\n",
       "      <td>-0.026846</td>\n",
       "      <td>-0.029687</td>\n",
       "      <td>0.030984</td>\n",
       "      <td>0.014489</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>-0.025575</td>\n",
       "      <td>-0.016180</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>155</td>\n",
       "      <td>-0.027310</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.060618</td>\n",
       "      <td>0.049415</td>\n",
       "      <td>0.085116</td>\n",
       "      <td>0.086368</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.037814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023104</td>\n",
       "      <td>0.056056</td>\n",
       "      <td>0.075937</td>\n",
       "      <td>0.029510</td>\n",
       "      <td>0.017894</td>\n",
       "      <td>0.010752</td>\n",
       "      <td>-0.001955</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>0.015606</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>156</td>\n",
       "      <td>-0.016412</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.010517</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.035760</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.021394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>0.022610</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>0.004828</td>\n",
       "      <td>-0.010755</td>\n",
       "      <td>0.008008</td>\n",
       "      <td>-0.006328</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>157</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.033151</td>\n",
       "      <td>-0.018294</td>\n",
       "      <td>0.031454</td>\n",
       "      <td>0.042840</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>0.019917</td>\n",
       "      <td>0.010226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010049</td>\n",
       "      <td>-0.006214</td>\n",
       "      <td>0.011544</td>\n",
       "      <td>0.026163</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>-0.023226</td>\n",
       "      <td>-0.007626</td>\n",
       "      <td>-0.015175</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>158</td>\n",
       "      <td>-0.012780</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.065486</td>\n",
       "      <td>-0.069938</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.016849</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>-0.007020</td>\n",
       "      <td>-0.030751</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025315</td>\n",
       "      <td>-0.027269</td>\n",
       "      <td>-0.032436</td>\n",
       "      <td>0.031763</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>0.017242</td>\n",
       "      <td>-0.022989</td>\n",
       "      <td>-0.011548</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>-0.005515</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.043373</td>\n",
       "      <td>0.087287</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.042345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023747</td>\n",
       "      <td>-0.009201</td>\n",
       "      <td>-0.016795</td>\n",
       "      <td>0.032060</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>0.019462</td>\n",
       "      <td>-0.029322</td>\n",
       "      <td>-0.017658</td>\n",
       "      <td>-0.035639</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     Col 1     Col 2     Col 3     Col 4     Col 5     Col 6  \\\n",
       "0      0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821   \n",
       "1      1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163   \n",
       "2      2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194   \n",
       "3      3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991   \n",
       "4      4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "155  155 -0.027310  0.050680  0.060618  0.049415  0.085116  0.086368   \n",
       "156  156 -0.016412 -0.044642 -0.010517  0.001215 -0.037344 -0.035760   \n",
       "157  157 -0.001882  0.050680 -0.033151 -0.018294  0.031454  0.042840   \n",
       "158  158 -0.012780 -0.044642 -0.065486 -0.069938  0.001183  0.016849   \n",
       "159  159 -0.005515 -0.044642  0.043373  0.087287  0.013567  0.007141   \n",
       "\n",
       "        Col 7     Col 8     Col 9  ...    Col 56    Col 57    Col 58  \\\n",
       "0   -0.043401 -0.002592  0.019908  ... -0.022038 -0.031125 -0.000922   \n",
       "1    0.074412 -0.039493 -0.068330  ... -0.011564  0.012973  0.023783   \n",
       "2   -0.032356 -0.002592  0.002864  ... -0.022063 -0.018016  0.004913   \n",
       "3   -0.036038  0.034309  0.022692  ... -0.009984 -0.003373 -0.019109   \n",
       "4    0.008142 -0.002592 -0.031991  ... -0.024091 -0.026846 -0.029687   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "155 -0.002903  0.034309  0.037814  ...  0.023104  0.056056  0.075937   \n",
       "156  0.011824 -0.039493 -0.021394  ... -0.001265  0.000986  0.012134   \n",
       "157 -0.013948  0.019917  0.010226  ... -0.010049 -0.006214  0.011544   \n",
       "158 -0.002903 -0.007020 -0.030751  ... -0.025315 -0.027269 -0.032436   \n",
       "159 -0.013948 -0.002592  0.042345  ... -0.023747 -0.009201 -0.016795   \n",
       "\n",
       "       Col 59    Col 60    Col 61    Col 62    Col 63    Col 64    y  \n",
       "0    0.033494  0.000852  0.031150 -0.028191 -0.017658 -0.027794  151  \n",
       "1   -0.023815 -0.094506 -0.140378  0.025298  0.053034  0.104013   75  \n",
       "2    0.032956  0.018281  0.032795 -0.027332 -0.017236 -0.022304  141  \n",
       "3    0.008159  0.001898  0.021514 -0.012045 -0.024872 -0.025042  206  \n",
       "4    0.030984  0.014489  0.005386 -0.025575 -0.016180  0.008735  135  \n",
       "..        ...       ...       ...       ...       ...       ...  ...  \n",
       "155  0.029510  0.017894  0.010752 -0.001955  0.014242  0.015606  186  \n",
       "156  0.022610  0.014659  0.004828 -0.010755  0.008008 -0.006328   25  \n",
       "157  0.026163  0.017152  0.005169 -0.023226 -0.007626 -0.015175   84  \n",
       "158  0.031763  0.022391  0.017242 -0.022989 -0.011548  0.010113   96  \n",
       "159  0.032060  0.007030  0.019462 -0.029322 -0.017658 -0.035639  195  \n",
       "\n",
       "[160 rows x 66 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = list(np.linspace(0, 100, 10))\n",
    "# alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.979154</td>\n",
       "      <td>1.119608</td>\n",
       "      <td>1.368607</td>\n",
       "      <td>0.574284</td>\n",
       "      <td>-0.889929</td>\n",
       "      <td>-0.664196</td>\n",
       "      <td>-0.951008</td>\n",
       "      <td>0.071018</td>\n",
       "      <td>0.595400</td>\n",
       "      <td>-0.262270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.910503</td>\n",
       "      <td>-0.384391</td>\n",
       "      <td>-0.602067</td>\n",
       "      <td>-0.001035</td>\n",
       "      <td>0.744189</td>\n",
       "      <td>-0.028493</td>\n",
       "      <td>0.603965</td>\n",
       "      <td>-0.578879</td>\n",
       "      <td>-0.361646</td>\n",
       "      <td>-0.545574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.145832</td>\n",
       "      <td>-0.893170</td>\n",
       "      <td>-1.097087</td>\n",
       "      <td>-0.479275</td>\n",
       "      <td>-0.056309</td>\n",
       "      <td>-0.318035</td>\n",
       "      <td>1.472666</td>\n",
       "      <td>-0.713171</td>\n",
       "      <td>-1.414235</td>\n",
       "      <td>-1.794395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.416689</td>\n",
       "      <td>-0.196079</td>\n",
       "      <td>0.415438</td>\n",
       "      <td>0.500077</td>\n",
       "      <td>-0.470420</td>\n",
       "      <td>-2.149835</td>\n",
       "      <td>-2.461611</td>\n",
       "      <td>0.783609</td>\n",
       "      <td>0.961210</td>\n",
       "      <td>2.221802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.963990</td>\n",
       "      <td>1.119608</td>\n",
       "      <td>0.992882</td>\n",
       "      <td>-0.027750</td>\n",
       "      <td>-0.921991</td>\n",
       "      <td>-0.650350</td>\n",
       "      <td>-0.723788</td>\n",
       "      <td>0.071018</td>\n",
       "      <td>0.207206</td>\n",
       "      <td>-0.432506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727568</td>\n",
       "      <td>-0.384849</td>\n",
       "      <td>-0.299607</td>\n",
       "      <td>0.117330</td>\n",
       "      <td>0.732792</td>\n",
       "      <td>0.359227</td>\n",
       "      <td>0.633364</td>\n",
       "      <td>-0.556989</td>\n",
       "      <td>-0.353745</td>\n",
       "      <td>-0.430309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.672327</td>\n",
       "      <td>-0.893170</td>\n",
       "      <td>-0.228223</td>\n",
       "      <td>-0.705038</td>\n",
       "      <td>0.424625</td>\n",
       "      <td>0.658138</td>\n",
       "      <td>-0.799528</td>\n",
       "      <td>0.855207</td>\n",
       "      <td>0.658797</td>\n",
       "      <td>-0.092034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179298</td>\n",
       "      <td>-0.167665</td>\n",
       "      <td>0.038271</td>\n",
       "      <td>-0.369933</td>\n",
       "      <td>0.207232</td>\n",
       "      <td>-0.005233</td>\n",
       "      <td>0.431742</td>\n",
       "      <td>-0.167610</td>\n",
       "      <td>-0.496642</td>\n",
       "      <td>-0.487800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.297345</td>\n",
       "      <td>-0.893170</td>\n",
       "      <td>-0.768328</td>\n",
       "      <td>0.574284</td>\n",
       "      <td>0.232251</td>\n",
       "      <td>0.450442</td>\n",
       "      <td>0.109349</td>\n",
       "      <td>0.071018</td>\n",
       "      <td>-0.586626</td>\n",
       "      <td>-0.858096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285019</td>\n",
       "      <td>-0.421313</td>\n",
       "      <td>-0.503356</td>\n",
       "      <td>-0.584494</td>\n",
       "      <td>0.691003</td>\n",
       "      <td>0.274877</td>\n",
       "      <td>0.143495</td>\n",
       "      <td>-0.512226</td>\n",
       "      <td>-0.333994</td>\n",
       "      <td>0.221374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-0.384465</td>\n",
       "      <td>1.119608</td>\n",
       "      <td>1.345124</td>\n",
       "      <td>1.176318</td>\n",
       "      <td>2.123926</td>\n",
       "      <td>2.015089</td>\n",
       "      <td>-0.117870</td>\n",
       "      <td>0.855207</td>\n",
       "      <td>1.003213</td>\n",
       "      <td>1.099619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114356</td>\n",
       "      <td>0.427244</td>\n",
       "      <td>1.409505</td>\n",
       "      <td>1.557934</td>\n",
       "      <td>0.659760</td>\n",
       "      <td>0.350628</td>\n",
       "      <td>0.239397</td>\n",
       "      <td>0.089429</td>\n",
       "      <td>0.235312</td>\n",
       "      <td>0.365625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.157195</td>\n",
       "      <td>-0.893170</td>\n",
       "      <td>-0.204741</td>\n",
       "      <td>0.122758</td>\n",
       "      <td>-0.729617</td>\n",
       "      <td>-0.684966</td>\n",
       "      <td>0.185089</td>\n",
       "      <td>-0.713171</td>\n",
       "      <td>-0.345260</td>\n",
       "      <td>-0.602742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036589</td>\n",
       "      <td>-0.010901</td>\n",
       "      <td>0.138848</td>\n",
       "      <td>0.263794</td>\n",
       "      <td>0.513524</td>\n",
       "      <td>0.278659</td>\n",
       "      <td>0.133531</td>\n",
       "      <td>-0.134729</td>\n",
       "      <td>0.118648</td>\n",
       "      <td>-0.094878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.145832</td>\n",
       "      <td>1.119608</td>\n",
       "      <td>-0.697880</td>\n",
       "      <td>-0.303682</td>\n",
       "      <td>0.873497</td>\n",
       "      <td>1.052762</td>\n",
       "      <td>-0.345089</td>\n",
       "      <td>0.549374</td>\n",
       "      <td>0.374874</td>\n",
       "      <td>0.674029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042351</td>\n",
       "      <td>-0.168834</td>\n",
       "      <td>-0.027281</td>\n",
       "      <td>0.251814</td>\n",
       "      <td>0.588820</td>\n",
       "      <td>0.334113</td>\n",
       "      <td>0.139626</td>\n",
       "      <td>-0.452410</td>\n",
       "      <td>-0.173910</td>\n",
       "      <td>-0.280645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-0.081438</td>\n",
       "      <td>-0.893170</td>\n",
       "      <td>-1.402364</td>\n",
       "      <td>-1.432496</td>\n",
       "      <td>0.168127</td>\n",
       "      <td>0.478135</td>\n",
       "      <td>-0.117870</td>\n",
       "      <td>-0.023085</td>\n",
       "      <td>-0.558379</td>\n",
       "      <td>-0.943214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205541</td>\n",
       "      <td>-0.443309</td>\n",
       "      <td>-0.513098</td>\n",
       "      <td>-0.640242</td>\n",
       "      <td>0.707517</td>\n",
       "      <td>0.450668</td>\n",
       "      <td>0.355386</td>\n",
       "      <td>-0.446358</td>\n",
       "      <td>-0.247313</td>\n",
       "      <td>0.250302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.070075</td>\n",
       "      <td>-0.893170</td>\n",
       "      <td>0.969399</td>\n",
       "      <td>2.004114</td>\n",
       "      <td>0.456687</td>\n",
       "      <td>0.263515</td>\n",
       "      <td>-0.345089</td>\n",
       "      <td>0.071018</td>\n",
       "      <td>1.106394</td>\n",
       "      <td>-0.262270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182635</td>\n",
       "      <td>-0.415121</td>\n",
       "      <td>-0.096215</td>\n",
       "      <td>-0.322997</td>\n",
       "      <td>0.713797</td>\n",
       "      <td>0.108939</td>\n",
       "      <td>0.395062</td>\n",
       "      <td>-0.607693</td>\n",
       "      <td>-0.361646</td>\n",
       "      <td>-0.710295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.979154  1.119608  1.368607  0.574284 -0.889929 -0.664196 -0.951008   \n",
       "1    0.145832 -0.893170 -1.097087 -0.479275 -0.056309 -0.318035  1.472666   \n",
       "2    1.963990  1.119608  0.992882 -0.027750 -0.921991 -0.650350 -0.723788   \n",
       "3   -1.672327 -0.893170 -0.228223 -0.705038  0.424625  0.658138 -0.799528   \n",
       "4    0.297345 -0.893170 -0.768328  0.574284  0.232251  0.450442  0.109349   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "155 -0.384465  1.119608  1.345124  1.176318  2.123926  2.015089 -0.117870   \n",
       "156 -0.157195 -0.893170 -0.204741  0.122758 -0.729617 -0.684966  0.185089   \n",
       "157  0.145832  1.119608 -0.697880 -0.303682  0.873497  1.052762 -0.345089   \n",
       "158 -0.081438 -0.893170 -1.402364 -1.432496  0.168127  0.478135 -0.117870   \n",
       "159  0.070075 -0.893170  0.969399  2.004114  0.456687  0.263515 -0.345089   \n",
       "\n",
       "           7         8         9   ...        54        55        56  \\\n",
       "0    0.071018  0.595400 -0.262270  ...  0.910503 -0.384391 -0.602067   \n",
       "1   -0.713171 -1.414235 -1.794395  ... -0.416689 -0.196079  0.415438   \n",
       "2    0.071018  0.207206 -0.432506  ...  0.727568 -0.384849 -0.299607   \n",
       "3    0.855207  0.658797 -0.092034  ... -0.179298 -0.167665  0.038271   \n",
       "4    0.071018 -0.586626 -0.858096  ...  0.285019 -0.421313 -0.503356   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "155  0.855207  1.003213  1.099619  ...  0.114356  0.427244  1.409505   \n",
       "156 -0.713171 -0.345260 -0.602742  ...  0.036589 -0.010901  0.138848   \n",
       "157  0.549374  0.374874  0.674029  ... -0.042351 -0.168834 -0.027281   \n",
       "158 -0.023085 -0.558379 -0.943214  ...  0.205541 -0.443309 -0.513098   \n",
       "159  0.071018  1.106394 -0.262270  ...  0.182635 -0.415121 -0.096215   \n",
       "\n",
       "           57        58        59        60        61        62        63  \n",
       "0   -0.001035  0.744189 -0.028493  0.603965 -0.578879 -0.361646 -0.545574  \n",
       "1    0.500077 -0.470420 -2.149835 -2.461611  0.783609  0.961210  2.221802  \n",
       "2    0.117330  0.732792  0.359227  0.633364 -0.556989 -0.353745 -0.430309  \n",
       "3   -0.369933  0.207232 -0.005233  0.431742 -0.167610 -0.496642 -0.487800  \n",
       "4   -0.584494  0.691003  0.274877  0.143495 -0.512226 -0.333994  0.221374  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "155  1.557934  0.659760  0.350628  0.239397  0.089429  0.235312  0.365625  \n",
       "156  0.263794  0.513524  0.278659  0.133531 -0.134729  0.118648 -0.094878  \n",
       "157  0.251814  0.588820  0.334113  0.139626 -0.452410 -0.173910 -0.280645  \n",
       "158 -0.640242  0.707517  0.450668  0.355386 -0.446358 -0.247313  0.250302  \n",
       "159 -0.322997  0.713797  0.108939  0.395062 -0.607693 -0.361646 -0.710295  \n",
       "\n",
       "[160 rows x 64 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop(columns=['id', 'y'])\n",
    "y = df['y']\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "X_df = pd.DataFrame(X)\n",
    "\n",
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 64) (40, 64)\n",
      "0      151\n",
      "1       75\n",
      "2      141\n",
      "3      206\n",
      "4      135\n",
      "      ... \n",
      "155    186\n",
      "156     25\n",
      "157     84\n",
      "158     96\n",
      "159    195\n",
      "Name: y, Length: 160, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, random_state=23)\n",
    "\n",
    "print(X_train.shape, X_val.shape)\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha 1.0, the train MSE is 1748.25, and the validation MSE is 3391.76, R^2 is 0.34\n",
      "For alpha 2.0, the train MSE is 1788.47, and the validation MSE is 3151.17, R^2 is 0.39\n",
      "For alpha 3.0, the train MSE is 1818.39, and the validation MSE is 2992.43, R^2 is 0.42\n",
      "For alpha 4.0, the train MSE is 1842.91, and the validation MSE is 2876.55, R^2 is 0.44\n",
      "For alpha 5.0, the train MSE is 1864.02, and the validation MSE is 2787.53, R^2 is 0.46\n",
      "For alpha 6.0, the train MSE is 1882.79, and the validation MSE is 2716.95, R^2 is 0.47\n",
      "For alpha 7.0, the train MSE is 1899.86, and the validation MSE is 2659.74, R^2 is 0.49\n",
      "For alpha 8.0, the train MSE is 1915.65, and the validation MSE is 2612.61, R^2 is 0.5\n",
      "For alpha 9.0, the train MSE is 1930.45, and the validation MSE is 2573.3, R^2 is 0.5\n",
      "For alpha 10.0, the train MSE is 1944.47, and the validation MSE is 2540.17, R^2 is 0.51\n",
      "For alpha 11.0, the train MSE is 1957.83, and the validation MSE is 2512.03, R^2 is 0.51\n",
      "For alpha 12.0, the train MSE is 1970.67, and the validation MSE is 2487.98, R^2 is 0.52\n",
      "For alpha 13.0, the train MSE is 1983.05, and the validation MSE is 2467.31, R^2 is 0.52\n",
      "For alpha 14.0, the train MSE is 1995.04, and the validation MSE is 2449.48, R^2 is 0.53\n",
      "For alpha 15.0, the train MSE is 2006.69, and the validation MSE is 2434.05, R^2 is 0.53\n",
      "For alpha 16.0, the train MSE is 2018.05, and the validation MSE is 2420.67, R^2 is 0.53\n",
      "For alpha 17.0, the train MSE is 2029.15, and the validation MSE is 2409.06, R^2 is 0.53\n",
      "For alpha 18.0, the train MSE is 2040.02, and the validation MSE is 2398.96, R^2 is 0.54\n",
      "For alpha 19.0, the train MSE is 2050.67, and the validation MSE is 2390.2, R^2 is 0.54\n",
      "For alpha 20.0, the train MSE is 2061.13, and the validation MSE is 2382.61, R^2 is 0.54\n",
      "For alpha 21.0, the train MSE is 2071.41, and the validation MSE is 2376.04, R^2 is 0.54\n",
      "For alpha 22.0, the train MSE is 2081.53, and the validation MSE is 2370.38, R^2 is 0.54\n",
      "For alpha 23.0, the train MSE is 2091.49, and the validation MSE is 2365.53, R^2 is 0.54\n",
      "For alpha 24.0, the train MSE is 2101.32, and the validation MSE is 2361.4, R^2 is 0.54\n",
      "For alpha 25.0, the train MSE is 2111.02, and the validation MSE is 2357.91, R^2 is 0.54\n",
      "For alpha 26.0, the train MSE is 2120.59, and the validation MSE is 2355.01, R^2 is 0.54\n",
      "For alpha 27.0, the train MSE is 2130.04, and the validation MSE is 2352.63, R^2 is 0.55\n",
      "For alpha 28.0, the train MSE is 2139.38, and the validation MSE is 2350.73, R^2 is 0.55\n",
      "For alpha 29.0, the train MSE is 2148.62, and the validation MSE is 2349.26, R^2 is 0.55\n",
      "For alpha 30.0, the train MSE is 2157.76, and the validation MSE is 2348.17, R^2 is 0.55\n",
      "For alpha 31.0, the train MSE is 2166.8, and the validation MSE is 2347.45, R^2 is 0.55\n",
      "For alpha 32.0, the train MSE is 2175.75, and the validation MSE is 2347.05, R^2 is 0.55\n",
      "For alpha 33.0, the train MSE is 2184.61, and the validation MSE is 2346.94, R^2 is 0.55\n",
      "For alpha 34.0, the train MSE is 2193.38, and the validation MSE is 2347.12, R^2 is 0.55\n",
      "For alpha 35.0, the train MSE is 2202.08, and the validation MSE is 2347.54, R^2 is 0.55\n",
      "For alpha 36.0, the train MSE is 2210.69, and the validation MSE is 2348.19, R^2 is 0.55\n",
      "For alpha 37.0, the train MSE is 2219.22, and the validation MSE is 2349.06, R^2 is 0.55\n",
      "For alpha 38.0, the train MSE is 2227.68, and the validation MSE is 2350.12, R^2 is 0.55\n",
      "For alpha 39.0, the train MSE is 2236.07, and the validation MSE is 2351.37, R^2 is 0.55\n",
      "For alpha 40.0, the train MSE is 2244.39, and the validation MSE is 2352.78, R^2 is 0.55\n",
      "For alpha 41.0, the train MSE is 2252.64, and the validation MSE is 2354.35, R^2 is 0.54\n",
      "For alpha 42.0, the train MSE is 2260.82, and the validation MSE is 2356.06, R^2 is 0.54\n",
      "For alpha 43.0, the train MSE is 2268.94, and the validation MSE is 2357.91, R^2 is 0.54\n",
      "For alpha 44.0, the train MSE is 2276.99, and the validation MSE is 2359.89, R^2 is 0.54\n",
      "For alpha 45.0, the train MSE is 2284.98, and the validation MSE is 2361.98, R^2 is 0.54\n",
      "For alpha 46.0, the train MSE is 2292.91, and the validation MSE is 2364.18, R^2 is 0.54\n",
      "For alpha 47.0, the train MSE is 2300.78, and the validation MSE is 2366.47, R^2 is 0.54\n",
      "For alpha 48.0, the train MSE is 2308.59, and the validation MSE is 2368.87, R^2 is 0.54\n",
      "For alpha 49.0, the train MSE is 2316.35, and the validation MSE is 2371.35, R^2 is 0.54\n",
      "For alpha 50.0, the train MSE is 2324.05, and the validation MSE is 2373.91, R^2 is 0.54\n",
      "For alpha 51.0, the train MSE is 2331.69, and the validation MSE is 2376.55, R^2 is 0.54\n",
      "For alpha 52.0, the train MSE is 2339.28, and the validation MSE is 2379.26, R^2 is 0.54\n",
      "For alpha 53.0, the train MSE is 2346.82, and the validation MSE is 2382.04, R^2 is 0.54\n",
      "For alpha 54.0, the train MSE is 2354.31, and the validation MSE is 2384.88, R^2 is 0.54\n",
      "For alpha 55.0, the train MSE is 2361.74, and the validation MSE is 2387.78, R^2 is 0.54\n",
      "For alpha 56.0, the train MSE is 2369.12, and the validation MSE is 2390.73, R^2 is 0.54\n",
      "For alpha 57.0, the train MSE is 2376.46, and the validation MSE is 2393.73, R^2 is 0.54\n",
      "For alpha 58.0, the train MSE is 2383.75, and the validation MSE is 2396.78, R^2 is 0.54\n",
      "For alpha 59.0, the train MSE is 2390.98, and the validation MSE is 2399.88, R^2 is 0.54\n",
      "For alpha 60.0, the train MSE is 2398.18, and the validation MSE is 2403.02, R^2 is 0.54\n",
      "For alpha 61.0, the train MSE is 2405.32, and the validation MSE is 2406.2, R^2 is 0.53\n",
      "For alpha 62.0, the train MSE is 2412.42, and the validation MSE is 2409.41, R^2 is 0.53\n",
      "For alpha 63.0, the train MSE is 2419.48, and the validation MSE is 2412.66, R^2 is 0.53\n",
      "For alpha 64.0, the train MSE is 2426.49, and the validation MSE is 2415.94, R^2 is 0.53\n",
      "For alpha 65.0, the train MSE is 2433.46, and the validation MSE is 2419.25, R^2 is 0.53\n",
      "For alpha 66.0, the train MSE is 2440.38, and the validation MSE is 2422.59, R^2 is 0.53\n",
      "For alpha 67.0, the train MSE is 2447.26, and the validation MSE is 2425.95, R^2 is 0.53\n",
      "For alpha 68.0, the train MSE is 2454.1, and the validation MSE is 2429.34, R^2 is 0.53\n",
      "For alpha 69.0, the train MSE is 2460.9, and the validation MSE is 2432.75, R^2 is 0.53\n",
      "For alpha 70.0, the train MSE is 2467.66, and the validation MSE is 2436.18, R^2 is 0.53\n",
      "For alpha 71.0, the train MSE is 2474.38, and the validation MSE is 2439.63, R^2 is 0.53\n",
      "For alpha 72.0, the train MSE is 2481.06, and the validation MSE is 2443.1, R^2 is 0.53\n",
      "For alpha 73.0, the train MSE is 2487.7, and the validation MSE is 2446.59, R^2 is 0.53\n",
      "For alpha 74.0, the train MSE is 2494.3, and the validation MSE is 2450.09, R^2 is 0.53\n",
      "For alpha 75.0, the train MSE is 2500.86, and the validation MSE is 2453.61, R^2 is 0.53\n",
      "For alpha 76.0, the train MSE is 2507.39, and the validation MSE is 2457.14, R^2 is 0.53\n",
      "For alpha 77.0, the train MSE is 2513.88, and the validation MSE is 2460.68, R^2 is 0.52\n",
      "For alpha 78.0, the train MSE is 2520.33, and the validation MSE is 2464.23, R^2 is 0.52\n",
      "For alpha 79.0, the train MSE is 2526.75, and the validation MSE is 2467.8, R^2 is 0.52\n",
      "For alpha 80.0, the train MSE is 2533.13, and the validation MSE is 2471.37, R^2 is 0.52\n",
      "For alpha 81.0, the train MSE is 2539.47, and the validation MSE is 2474.95, R^2 is 0.52\n",
      "For alpha 82.0, the train MSE is 2545.78, and the validation MSE is 2478.54, R^2 is 0.52\n",
      "For alpha 83.0, the train MSE is 2552.06, and the validation MSE is 2482.14, R^2 is 0.52\n",
      "For alpha 84.0, the train MSE is 2558.3, and the validation MSE is 2485.74, R^2 is 0.52\n",
      "For alpha 85.0, the train MSE is 2564.5, and the validation MSE is 2489.35, R^2 is 0.52\n",
      "For alpha 86.0, the train MSE is 2570.68, and the validation MSE is 2492.97, R^2 is 0.52\n",
      "For alpha 87.0, the train MSE is 2576.82, and the validation MSE is 2496.59, R^2 is 0.52\n",
      "For alpha 88.0, the train MSE is 2582.93, and the validation MSE is 2500.21, R^2 is 0.52\n",
      "For alpha 89.0, the train MSE is 2589.0, and the validation MSE is 2503.83, R^2 is 0.52\n",
      "For alpha 90.0, the train MSE is 2595.05, and the validation MSE is 2507.46, R^2 is 0.52\n",
      "For alpha 91.0, the train MSE is 2601.06, and the validation MSE is 2511.09, R^2 is 0.51\n",
      "For alpha 92.0, the train MSE is 2607.04, and the validation MSE is 2514.72, R^2 is 0.51\n",
      "For alpha 93.0, the train MSE is 2612.99, and the validation MSE is 2518.35, R^2 is 0.51\n",
      "For alpha 94.0, the train MSE is 2618.91, and the validation MSE is 2521.99, R^2 is 0.51\n",
      "For alpha 95.0, the train MSE is 2624.8, and the validation MSE is 2525.62, R^2 is 0.51\n",
      "For alpha 96.0, the train MSE is 2630.67, and the validation MSE is 2529.25, R^2 is 0.51\n",
      "For alpha 97.0, the train MSE is 2636.5, and the validation MSE is 2532.89, R^2 is 0.51\n",
      "For alpha 98.0, the train MSE is 2642.3, and the validation MSE is 2536.52, R^2 is 0.51\n",
      "For alpha 99.0, the train MSE is 2648.07, and the validation MSE is 2540.15, R^2 is 0.51\n",
      "For alpha 100.0, the train MSE is 2653.81, and the validation MSE is 2543.78, R^2 is 0.51\n",
      "Best alpha: 33.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "alpha_values = np.round(np.linspace(1, 100, 100), 0)\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "r2_scores = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the training and validation sets\n",
    "    y_pred_train = ridge.predict(X_train)\n",
    "    y_pred_val = ridge.predict(X_val)\n",
    "    \n",
    "    # Calculate MSE and R^2 for training and validation sets\n",
    "    train_mse.append(np.round(mean_squared_error(y_train, y_pred_train), 2))\n",
    "    val_mse.append(np.round(mean_squared_error(y_val, y_pred_val), 2))\n",
    "    r2_scores.append(np.round(r2_score(y_val, y_pred_val), 2))\n",
    "    \n",
    "    print(f'For alpha {alpha}, the train MSE is {train_mse[-1]}, and the validation MSE is {val_mse[-1]}, R^2 is {r2_scores[-1]}')\n",
    "\n",
    "# Find the alpha with the lowest validation MSE\n",
    "best_alpha = alpha_values[np.argmin(val_mse)]\n",
    "print(f'Best alpha: {best_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 74.0\n",
      "Validation MSE: 2450.091703383112\n",
      "R^2 score on validation set: 0.5264539011776506\n"
     ]
    }
   ],
   "source": [
    "# RidgeCV\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alpha_values, cv=5)  # Create a RidgeCV model with cross-validation\n",
    "ridge_cv.fit(X_train, y_train)  # Fit the model to your training data\n",
    "\n",
    "# Access the best alpha and coefficients\n",
    "best_alpha = ridge_cv.alpha_\n",
    "coefficients = ridge_cv.coef_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_val = ridge_cv.predict(X_val)\n",
    "\n",
    "# Calculate MSE and R^2 for validation set\n",
    "val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "r2 = r2_score(y_val, y_pred_val)\n",
    "\n",
    "print(f'Best alpha: {best_alpha}')\n",
    "print(f'Validation MSE: {val_mse}')\n",
    "print(f'R^2 score on validation set: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge(alpha=50)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge final model\n",
    "\n",
    "model = Ridge(alpha = 50)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 3.0\n",
      "Validation MSE: 2446.8318633227263\n",
      "R^2 score on validation set: 0.5270839529186728\n"
     ]
    }
   ],
   "source": [
    "# LassoCV\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso_cv = LassoCV(alphas=alpha_values, cv=5)  # Create a LassoCV model with cross-validation\n",
    "lasso_cv.fit(X_train, y_train)  # Fit the model to your training data\n",
    "\n",
    "# Access the best alpha and coefficients\n",
    "best_alpha = lasso_cv.alpha_\n",
    "coefficients = lasso_cv.coef_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_val = lasso_cv.predict(X_val)\n",
    "\n",
    "# Calculate MSE and R^2 for validation set\n",
    "val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "r2 = r2_score(y_val, y_pred_val)\n",
    "\n",
    "print(f'Best alpha: {best_alpha}')\n",
    "print(f'Validation MSE: {val_mse}')\n",
    "print(f'R^2 score on validation set: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Lasso(alpha=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso(alpha=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Lasso(alpha=3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso final model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha = 3)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160 entries, 0 to 159\n",
      "Data columns (total 66 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      160 non-null    int64  \n",
      " 1   Col 1   160 non-null    float64\n",
      " 2   Col 2   160 non-null    float64\n",
      " 3   Col 3   160 non-null    float64\n",
      " 4   Col 4   160 non-null    float64\n",
      " 5   Col 5   160 non-null    float64\n",
      " 6   Col 6   160 non-null    float64\n",
      " 7   Col 7   160 non-null    float64\n",
      " 8   Col 8   160 non-null    float64\n",
      " 9   Col 9   160 non-null    float64\n",
      " 10  Col 10  160 non-null    float64\n",
      " 11  Col 11  160 non-null    float64\n",
      " 12  Col 12  160 non-null    float64\n",
      " 13  Col 13  160 non-null    float64\n",
      " 14  Col 14  160 non-null    float64\n",
      " 15  Col 15  160 non-null    float64\n",
      " 16  Col 16  160 non-null    float64\n",
      " 17  Col 17  160 non-null    float64\n",
      " 18  Col 18  160 non-null    float64\n",
      " 19  Col 19  160 non-null    float64\n",
      " 20  Col 20  160 non-null    float64\n",
      " 21  Col 21  160 non-null    float64\n",
      " 22  Col 22  160 non-null    float64\n",
      " 23  Col 23  160 non-null    float64\n",
      " 24  Col 24  160 non-null    float64\n",
      " 25  Col 25  160 non-null    float64\n",
      " 26  Col 26  160 non-null    float64\n",
      " 27  Col 27  160 non-null    float64\n",
      " 28  Col 28  160 non-null    float64\n",
      " 29  Col 29  160 non-null    float64\n",
      " 30  Col 30  160 non-null    float64\n",
      " 31  Col 31  160 non-null    float64\n",
      " 32  Col 32  160 non-null    float64\n",
      " 33  Col 33  160 non-null    float64\n",
      " 34  Col 34  160 non-null    float64\n",
      " 35  Col 35  160 non-null    float64\n",
      " 36  Col 36  160 non-null    float64\n",
      " 37  Col 37  160 non-null    float64\n",
      " 38  Col 38  160 non-null    float64\n",
      " 39  Col 39  160 non-null    float64\n",
      " 40  Col 40  160 non-null    float64\n",
      " 41  Col 41  160 non-null    float64\n",
      " 42  Col 42  160 non-null    float64\n",
      " 43  Col 43  160 non-null    float64\n",
      " 44  Col 44  160 non-null    float64\n",
      " 45  Col 45  160 non-null    float64\n",
      " 46  Col 46  160 non-null    float64\n",
      " 47  Col 47  160 non-null    float64\n",
      " 48  Col 48  160 non-null    float64\n",
      " 49  Col 49  160 non-null    float64\n",
      " 50  Col 50  160 non-null    float64\n",
      " 51  Col 51  160 non-null    float64\n",
      " 52  Col 52  160 non-null    float64\n",
      " 53  Col 53  160 non-null    float64\n",
      " 54  Col 54  160 non-null    float64\n",
      " 55  Col 55  160 non-null    float64\n",
      " 56  Col 56  160 non-null    float64\n",
      " 57  Col 57  160 non-null    float64\n",
      " 58  Col 58  160 non-null    float64\n",
      " 59  Col 59  160 non-null    float64\n",
      " 60  Col 60  160 non-null    float64\n",
      " 61  Col 61  160 non-null    float64\n",
      " 62  Col 62  160 non-null    float64\n",
      " 63  Col 63  160 non-null    float64\n",
      " 64  Col 64  160 non-null    float64\n",
      " 65  y       160 non-null    int64  \n",
      "dtypes: float64(64), int64(2)\n",
      "memory usage: 82.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45      53\n",
       "102    302\n",
       "72     202\n",
       "136     85\n",
       "67      97\n",
       "      ... \n",
       "39      90\n",
       "91     164\n",
       "31      59\n",
       "40     100\n",
       "83     210\n",
       "Name: y, Length: 120, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "Col 1     0\n",
       "Col 2     0\n",
       "Col 3     0\n",
       "Col 4     0\n",
       "         ..\n",
       "Col 61    0\n",
       "Col 62    0\n",
       "Col 63    0\n",
       "Col 64    0\n",
       "y         0\n",
       "Length: 66, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      151\n",
       "1       75\n",
       "2      141\n",
       "3      206\n",
       "4      135\n",
       "      ... \n",
       "155    186\n",
       "156     25\n",
       "157     84\n",
       "158     96\n",
       "159    195\n",
       "Name: y, Length: 160, dtype: int64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([15., 26., 29., 24., 21., 14.,  6., 12.,  9.,  4.]),\n",
       " array([ 25. ,  56.6,  88.2, 119.8, 151.4, 183. , 214.6, 246.2, 277.8,\n",
       "        309.4, 341. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Col 1</th>\n",
       "      <th>Col 2</th>\n",
       "      <th>Col 3</th>\n",
       "      <th>Col 4</th>\n",
       "      <th>Col 5</th>\n",
       "      <th>Col 6</th>\n",
       "      <th>Col 7</th>\n",
       "      <th>Col 8</th>\n",
       "      <th>Col 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Col 56</th>\n",
       "      <th>Col 57</th>\n",
       "      <th>Col 58</th>\n",
       "      <th>Col 59</th>\n",
       "      <th>Col 60</th>\n",
       "      <th>Col 61</th>\n",
       "      <th>Col 62</th>\n",
       "      <th>Col 63</th>\n",
       "      <th>Col 64</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>79.500000</td>\n",
       "      <td>-0.008875</td>\n",
       "      <td>-0.002343</td>\n",
       "      <td>-0.001120</td>\n",
       "      <td>-0.004401</td>\n",
       "      <td>-0.006032</td>\n",
       "      <td>-0.004778</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>-0.005934</td>\n",
       "      <td>-0.006234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>-0.005031</td>\n",
       "      <td>-0.000871</td>\n",
       "      <td>-0.001619</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>-0.002643</td>\n",
       "      <td>-0.005465</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>-0.001809</td>\n",
       "      <td>147.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>46.332134</td>\n",
       "      <td>0.048101</td>\n",
       "      <td>0.047507</td>\n",
       "      <td>0.046042</td>\n",
       "      <td>0.045894</td>\n",
       "      <td>0.043050</td>\n",
       "      <td>0.045374</td>\n",
       "      <td>0.048762</td>\n",
       "      <td>0.047204</td>\n",
       "      <td>0.044045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055793</td>\n",
       "      <td>0.043475</td>\n",
       "      <td>0.049456</td>\n",
       "      <td>0.047331</td>\n",
       "      <td>0.045093</td>\n",
       "      <td>0.056129</td>\n",
       "      <td>0.039382</td>\n",
       "      <td>0.053606</td>\n",
       "      <td>0.047778</td>\n",
       "      <td>76.364695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.107226</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.083808</td>\n",
       "      <td>-0.108957</td>\n",
       "      <td>-0.126781</td>\n",
       "      <td>-0.106845</td>\n",
       "      <td>-0.102307</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>-0.126097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076433</td>\n",
       "      <td>-0.155145</td>\n",
       "      <td>-0.151734</td>\n",
       "      <td>-0.228580</td>\n",
       "      <td>-0.143972</td>\n",
       "      <td>-0.223255</td>\n",
       "      <td>-0.160745</td>\n",
       "      <td>-0.128919</td>\n",
       "      <td>-0.092165</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.750000</td>\n",
       "      <td>-0.046381</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.035307</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.034273</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.034524</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023703</td>\n",
       "      <td>-0.022007</td>\n",
       "      <td>-0.020446</td>\n",
       "      <td>-0.021084</td>\n",
       "      <td>-0.018638</td>\n",
       "      <td>-0.017585</td>\n",
       "      <td>-0.026875</td>\n",
       "      <td>-0.019626</td>\n",
       "      <td>-0.023964</td>\n",
       "      <td>87.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>79.500000</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.006206</td>\n",
       "      <td>-0.012556</td>\n",
       "      <td>-0.005009</td>\n",
       "      <td>-0.009925</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014868</td>\n",
       "      <td>-0.009495</td>\n",
       "      <td>-0.010498</td>\n",
       "      <td>0.014418</td>\n",
       "      <td>0.009642</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>-0.014899</td>\n",
       "      <td>-0.015336</td>\n",
       "      <td>-0.015265</td>\n",
       "      <td>134.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>119.250000</td>\n",
       "      <td>0.030811</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.028284</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.020446</td>\n",
       "      <td>0.021703</td>\n",
       "      <td>0.037595</td>\n",
       "      <td>0.025822</td>\n",
       "      <td>0.026221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003447</td>\n",
       "      <td>0.013173</td>\n",
       "      <td>0.009096</td>\n",
       "      <td>0.031522</td>\n",
       "      <td>0.022619</td>\n",
       "      <td>0.022176</td>\n",
       "      <td>0.014464</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.011668</td>\n",
       "      <td>195.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>159.000000</td>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.128521</td>\n",
       "      <td>0.125158</td>\n",
       "      <td>0.152538</td>\n",
       "      <td>0.198788</td>\n",
       "      <td>0.181179</td>\n",
       "      <td>0.185234</td>\n",
       "      <td>0.133599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555129</td>\n",
       "      <td>0.113164</td>\n",
       "      <td>0.299032</td>\n",
       "      <td>0.080445</td>\n",
       "      <td>0.163067</td>\n",
       "      <td>0.209905</td>\n",
       "      <td>0.157844</td>\n",
       "      <td>0.318104</td>\n",
       "      <td>0.338184</td>\n",
       "      <td>341.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       Col 1       Col 2       Col 3       Col 4       Col 5  \\\n",
       "count  160.000000  160.000000  160.000000  160.000000  160.000000  160.000000   \n",
       "mean    79.500000   -0.008875   -0.002343   -0.001120   -0.004401   -0.006032   \n",
       "std     46.332134    0.048101    0.047507    0.046042    0.045894    0.043050   \n",
       "min      0.000000   -0.107226   -0.044642   -0.083808   -0.108957   -0.126781   \n",
       "25%     39.750000   -0.046381   -0.044642   -0.035307   -0.036656   -0.037344   \n",
       "50%     79.500000   -0.001882   -0.044642   -0.006206   -0.012556   -0.005009   \n",
       "75%    119.250000    0.030811    0.050680    0.028284    0.021872    0.020446   \n",
       "max    159.000000    0.085299    0.050680    0.128521    0.125158    0.152538   \n",
       "\n",
       "            Col 6       Col 7       Col 8       Col 9  ...      Col 56  \\\n",
       "count  160.000000  160.000000  160.000000  160.000000  ...  160.000000   \n",
       "mean    -0.004778    0.002827   -0.005934   -0.006234  ...   -0.000659   \n",
       "std      0.045374    0.048762    0.047204    0.044045  ...    0.055793   \n",
       "min     -0.106845   -0.102307   -0.076395   -0.126097  ...   -0.076433   \n",
       "25%     -0.034273   -0.032356   -0.039493   -0.034524  ...   -0.023703   \n",
       "50%     -0.009925   -0.002903   -0.002592   -0.010412  ...   -0.014868   \n",
       "75%      0.021703    0.037595    0.025822    0.026221  ...    0.003447   \n",
       "max      0.198788    0.181179    0.185234    0.133599  ...    0.555129   \n",
       "\n",
       "           Col 57      Col 58      Col 59      Col 60      Col 61      Col 62  \\\n",
       "count  160.000000  160.000000  160.000000  160.000000  160.000000  160.000000   \n",
       "mean    -0.005031   -0.000871   -0.001619    0.002133   -0.002643   -0.005465   \n",
       "std      0.043475    0.049456    0.047331    0.045093    0.056129    0.039382   \n",
       "min     -0.155145   -0.151734   -0.228580   -0.143972   -0.223255   -0.160745   \n",
       "25%     -0.022007   -0.020446   -0.021084   -0.018638   -0.017585   -0.026875   \n",
       "50%     -0.009495   -0.010498    0.014418    0.009642    0.009252   -0.014899   \n",
       "75%      0.013173    0.009096    0.031522    0.022619    0.022176    0.014464   \n",
       "max      0.113164    0.299032    0.080445    0.163067    0.209905    0.157844   \n",
       "\n",
       "           Col 63      Col 64           y  \n",
       "count  160.000000  160.000000  160.000000  \n",
       "mean     0.001668   -0.001809  147.843750  \n",
       "std      0.053606    0.047778   76.364695  \n",
       "min     -0.128919   -0.092165   25.000000  \n",
       "25%     -0.019626   -0.023964   87.750000  \n",
       "50%     -0.015336   -0.015265  134.500000  \n",
       "75%      0.018200    0.011668  195.500000  \n",
       "max      0.318104    0.338184  341.000000  \n",
       "\n",
       "[8 rows x 66 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['y'])\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 2)\n",
      "(32, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=23)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5107.34"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "np.round(mse, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find best principal components by mse score\n",
    "\n",
    "# Intialize mse\n",
    "mse = []\n",
    "num_comps = []\n",
    "\n",
    "for comp in range(1, len(X.columns)):\n",
    "    pca = PCA(n_components=comp)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=23)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_test)\n",
    "    mse.append(mean_squared_error(y_test, y_pred))\n",
    "    num_comps.append(comp)\n",
    "\n",
    "plt.scatter(num_comps, mse)\n",
    "plt.plot(num_comps, mse)\n",
    "plt.title(\"MSE by Principal Components\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of principal components: 20\n",
      "Best MSE: 2345.448194670188\n"
     ]
    }
   ],
   "source": [
    "best_mse = min(mse)\n",
    "best_num_comp = np.argmin(mse)+1\n",
    "\n",
    "print(f'Best number of principal components: {best_num_comp}\\nBest MSE: {best_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop(columns=(['id', 'y']))\n",
    "y = df['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5702.131169565064\n",
      "-0.20950829219659206\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(mse)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mse of 5702 is baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# # Create a heatmap\n",
    "# plt.figure(figsize=(40, 12))\n",
    "# sns.heatmap(X.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 50}\n",
      "Mean Squared Error: 3555.3357608149126\n",
      "0.2458594942184188\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a KFold object for cross-validation\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=23)\n",
    "\n",
    "# Create a RandomForestRegressor instance\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('rf', rf)\n",
    "])\n",
    "\n",
    "# Create GridSearchCV with the pipeline\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=k_fold, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding mean squared error\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, min_samples_split=10, n_estimators=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, min_samples_split=10, n_estimators=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10, min_samples_split=10, n_estimators=50)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final fit\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=best_params['rf__n_estimators'],  # Replace with your best hyperparameters\n",
    "    max_depth=best_params['rf__max_depth'],\n",
    "    min_samples_split=best_params['rf__min_samples_split'],\n",
    "    min_samples_leaf=best_params['rf__min_samples_leaf']\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)  # Replace with your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 3144.3486130357396\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)  # Replace with your testing data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on Test Data:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 3408.695740883035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(\n",
    "    n_estimators=100,             # Number of boosting rounds (trees)\n",
    "    max_depth=3,                  # Maximum tree depth\n",
    "    learning_rate=0.1,            # Learning rate\n",
    "    random_state=42               # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model to your training data\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance on the test data (optional)\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on Test Data:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "MSE: 3133.9894971384883\n",
      "R-squared: 0.3352334115570407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.005, 0.01, 0.1],\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'min_samples_split': [5, 10, 15],\n",
    "}\n",
    "\n",
    "# Perfrom grid search\n",
    "\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingRegressor(),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error',  # Use negative mean squared error for regression\n",
    "                           cv=5,  # Cross-validation folds\n",
    "                           n_jobs=-1)  # Use all available CPU cores\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Able to account for around 33% of explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=2, min_samples_split=5,\n",
       "                          n_estimators=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=2, min_samples_split=5,\n",
       "                          n_estimators=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.01, max_depth=2, min_samples_split=5,\n",
       "                          n_estimators=200)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13514621, -0.00474979,  0.02583257, ..., -0.0084823 ,\n",
       "        -0.01897076,  0.01031128],\n",
       "       [ 0.10958537, -0.04190665,  0.23006124, ..., -0.02666256,\n",
       "         0.01975327,  0.00815665],\n",
       "       [-0.13193769, -0.00900334, -0.01233844, ..., -0.01139219,\n",
       "        -0.06182718,  0.02191633],\n",
       "       ...,\n",
       "       [-0.08842229,  0.07198216, -0.04962685, ...,  0.02156718,\n",
       "        -0.00356099,  0.01186815],\n",
       "       [-0.04012474, -0.0893297 ,  0.04881715, ...,  0.03274057,\n",
       "         0.04612079, -0.04532398],\n",
       "       [-0.11226846,  0.00593371, -0.01697215, ..., -0.01264661,\n",
       "         0.02143295,  0.01852741]])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pca = PCA(n_components=20)\n",
    "# X_pca = pca.fit_transform(X)\n",
    "\n",
    "# X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col 1</th>\n",
       "      <th>Col 2</th>\n",
       "      <th>Col 3</th>\n",
       "      <th>Col 4</th>\n",
       "      <th>Col 5</th>\n",
       "      <th>Col 6</th>\n",
       "      <th>Col 7</th>\n",
       "      <th>Col 8</th>\n",
       "      <th>Col 9</th>\n",
       "      <th>Col 10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC11</th>\n",
       "      <th>PC12</th>\n",
       "      <th>PC13</th>\n",
       "      <th>PC14</th>\n",
       "      <th>PC15</th>\n",
       "      <th>PC16</th>\n",
       "      <th>PC17</th>\n",
       "      <th>PC18</th>\n",
       "      <th>PC19</th>\n",
       "      <th>PC20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>-0.039228</td>\n",
       "      <td>-0.015740</td>\n",
       "      <td>-0.025075</td>\n",
       "      <td>0.053813</td>\n",
       "      <td>0.025636</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>-0.008482</td>\n",
       "      <td>-0.018971</td>\n",
       "      <td>0.010311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045855</td>\n",
       "      <td>-0.047766</td>\n",
       "      <td>-0.036022</td>\n",
       "      <td>-0.039965</td>\n",
       "      <td>-0.062375</td>\n",
       "      <td>-0.023645</td>\n",
       "      <td>0.048946</td>\n",
       "      <td>-0.026663</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>0.008157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020876</td>\n",
       "      <td>-0.010511</td>\n",
       "      <td>-0.046361</td>\n",
       "      <td>-0.080511</td>\n",
       "      <td>0.059445</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.011291</td>\n",
       "      <td>-0.011392</td>\n",
       "      <td>-0.061827</td>\n",
       "      <td>0.021916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059722</td>\n",
       "      <td>0.123671</td>\n",
       "      <td>-0.064285</td>\n",
       "      <td>0.088771</td>\n",
       "      <td>0.036069</td>\n",
       "      <td>-0.064199</td>\n",
       "      <td>-0.017148</td>\n",
       "      <td>0.025608</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>0.037246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024569</td>\n",
       "      <td>-0.008119</td>\n",
       "      <td>-0.017292</td>\n",
       "      <td>-0.034439</td>\n",
       "      <td>-0.046257</td>\n",
       "      <td>-0.015562</td>\n",
       "      <td>0.013945</td>\n",
       "      <td>-0.013426</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>-0.023829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-0.027310</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.060618</td>\n",
       "      <td>0.049415</td>\n",
       "      <td>0.085116</td>\n",
       "      <td>0.086368</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.037814</td>\n",
       "      <td>0.048628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>-0.043209</td>\n",
       "      <td>-0.052963</td>\n",
       "      <td>-0.041562</td>\n",
       "      <td>-0.109108</td>\n",
       "      <td>-0.054037</td>\n",
       "      <td>-0.080887</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>0.032465</td>\n",
       "      <td>0.066073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.016412</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.010517</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.035760</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.021394</td>\n",
       "      <td>-0.034215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013241</td>\n",
       "      <td>-0.037061</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>-0.004769</td>\n",
       "      <td>-0.038225</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>-0.004667</td>\n",
       "      <td>-0.006226</td>\n",
       "      <td>-0.005973</td>\n",
       "      <td>-0.031314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.033151</td>\n",
       "      <td>-0.018294</td>\n",
       "      <td>0.031454</td>\n",
       "      <td>0.042840</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>0.019917</td>\n",
       "      <td>0.010226</td>\n",
       "      <td>0.027917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>-0.012756</td>\n",
       "      <td>0.008345</td>\n",
       "      <td>-0.026326</td>\n",
       "      <td>-0.028209</td>\n",
       "      <td>0.004526</td>\n",
       "      <td>-0.004450</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>0.011868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-0.012780</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.065486</td>\n",
       "      <td>-0.069938</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.016849</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>-0.007020</td>\n",
       "      <td>-0.030751</td>\n",
       "      <td>-0.050783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034837</td>\n",
       "      <td>0.049515</td>\n",
       "      <td>-0.027792</td>\n",
       "      <td>-0.015483</td>\n",
       "      <td>-0.001802</td>\n",
       "      <td>-0.053960</td>\n",
       "      <td>0.019541</td>\n",
       "      <td>0.032741</td>\n",
       "      <td>0.046121</td>\n",
       "      <td>-0.045324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.043373</td>\n",
       "      <td>0.087287</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.042345</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098882</td>\n",
       "      <td>-0.024422</td>\n",
       "      <td>0.071943</td>\n",
       "      <td>0.040445</td>\n",
       "      <td>-0.037968</td>\n",
       "      <td>-0.016229</td>\n",
       "      <td>0.012809</td>\n",
       "      <td>-0.012647</td>\n",
       "      <td>0.021433</td>\n",
       "      <td>0.018527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Col 1     Col 2     Col 3     Col 4     Col 5     Col 6     Col 7  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "155 -0.027310  0.050680  0.060618  0.049415  0.085116  0.086368 -0.002903   \n",
       "156 -0.016412 -0.044642 -0.010517  0.001215 -0.037344 -0.035760  0.011824   \n",
       "157 -0.001882  0.050680 -0.033151 -0.018294  0.031454  0.042840 -0.013948   \n",
       "158 -0.012780 -0.044642 -0.065486 -0.069938  0.001183  0.016849 -0.002903   \n",
       "159 -0.005515 -0.044642  0.043373  0.087287  0.013567  0.007141 -0.013948   \n",
       "\n",
       "        Col 8     Col 9    Col 10  ...      PC11      PC12      PC13  \\\n",
       "0   -0.002592  0.019908 -0.017646  ...  0.022265 -0.039228 -0.015740   \n",
       "1   -0.039493 -0.068330 -0.092204  ...  0.045855 -0.047766 -0.036022   \n",
       "2   -0.002592  0.002864 -0.025930  ...  0.020876 -0.010511 -0.046361   \n",
       "3    0.034309  0.022692 -0.009362  ...  0.059722  0.123671 -0.064285   \n",
       "4   -0.002592 -0.031991 -0.046641  ...  0.024569 -0.008119 -0.017292   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "155  0.034309  0.037814  0.048628  ...  0.009934 -0.043209 -0.052963   \n",
       "156 -0.039493 -0.021394 -0.034215  ...  0.013241 -0.037061  0.002724   \n",
       "157  0.019917  0.010226  0.027917  ...  0.005283 -0.012756  0.008345   \n",
       "158 -0.007020 -0.030751 -0.050783  ...  0.034837  0.049515 -0.027792   \n",
       "159 -0.002592  0.042345 -0.017646  ... -0.098882 -0.024422  0.071943   \n",
       "\n",
       "         PC14      PC15      PC16      PC17      PC18      PC19      PC20  \n",
       "0   -0.025075  0.053813  0.025636  0.004854 -0.008482 -0.018971  0.010311  \n",
       "1   -0.039965 -0.062375 -0.023645  0.048946 -0.026663  0.019753  0.008157  \n",
       "2   -0.080511  0.059445  0.000677  0.011291 -0.011392 -0.061827  0.021916  \n",
       "3    0.088771  0.036069 -0.064199 -0.017148  0.025608 -0.035049  0.037246  \n",
       "4   -0.034439 -0.046257 -0.015562  0.013945 -0.013426  0.035698 -0.023829  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "155 -0.041562 -0.109108 -0.054037 -0.080887 -0.000646  0.032465  0.066073  \n",
       "156 -0.004769 -0.038225  0.025572 -0.004667 -0.006226 -0.005973 -0.031314  \n",
       "157 -0.026326 -0.028209  0.004526 -0.004450  0.021567 -0.003561  0.011868  \n",
       "158 -0.015483 -0.001802 -0.053960  0.019541  0.032741  0.046121 -0.045324  \n",
       "159  0.040445 -0.037968 -0.016229  0.012809 -0.012647  0.021433  0.018527  \n",
       "\n",
       "[160 rows x 84 columns]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create a DataFrame for the principal components\n",
    "# pca_df = pd.DataFrame(data=X_pca, columns=[f'PC{i+1}' for i in range(20)])\n",
    "\n",
    "# # Concatenate the PCA DataFrame with your original DataFrame X\n",
    "# X_with_pca = pd.concat([X, pca_df], axis=1)\n",
    "\n",
    "# X_with_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with_pca, y, test_size=0.2, random_state=23)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 2, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "MSE: 3118.6070544717363\n",
      "R-squared: 0.3384962603773167\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'learning_rate': [0.005, 0.01, 0.1],\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'min_samples_split': [5, 10, 15],\n",
    "}\n",
    "\n",
    "# Perfrom grid search\n",
    "\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingRegressor(),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error',  # Use negative mean squared error for regression\n",
    "                           cv=5,  # Cross-validation folds\n",
    "                           n_jobs=-1)  # Use all available CPU cores\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=2, min_samples_split=5,\n",
       "                          n_estimators=300)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=2, min_samples_split=5,\n",
       "                          n_estimators=300)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.01, max_depth=2, min_samples_split=5,\n",
       "                          n_estimators=300)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "\n",
    "x_test_kaggle = pd.read_csv('x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bdogellis/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but Lasso was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# drop ID\n",
    "\n",
    "x_test_kaggle_drop = x_test_kaggle.drop('id', axis=1)\n",
    "\n",
    "# Get predictions \n",
    "\n",
    "y_pred_kaggle = model.predict(x_test_kaggle_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df = pd.read_csv('sample_submission.csv')\n",
    "kaggle_df['y'] = y_pred_kaggle\n",
    "\n",
    "\n",
    "# kaggle_df.to_csv('submission.csv', index=False)\n",
    "kaggle_df = kaggle_df[['id', 'y']]\n",
    "kaggle_df.to_csv('brian_submission_lasso.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cde9e06954608812f36a56132da0251c351f6ad8984d203ba87e4f78021e1e3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
