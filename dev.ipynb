{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.1)\n",
      "R2 Score:  0.4778632052932068\n",
      "MSE:  2597.8243153674566\n",
      "CV Score R2:  0.3825445230582888\n",
      "CV Score MSE:  3669.9434656308963\n",
      "-------------------------------------------\n",
      "RandomForestRegressor(max_depth=4, max_features='auto', min_samples_leaf=5,\n",
      "                      min_samples_split=3, n_estimators=311)\n",
      "R2 Score:  0.35319620748222946\n",
      "MSE:  3218.088892620788\n",
      "CV Score R2:  0.40201606839922865\n",
      "CV Score MSE:  3473.483656161877\n",
      "-------------------------------------------\n",
      "GradientBoostingRegressor(learning_rate=0.01, max_depth=1, min_samples_split=15,\n",
      "                          n_estimators=350)\n",
      "R2 Score:  0.4208316159632409\n",
      "MSE:  2881.57763635039\n",
      "CV Score R2:  0.3743878054734526\n",
      "CV Score MSE:  3679.8105224847204\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data = pd.read_csv('train.csv')\n",
    "# data['Col 2'] = data['Col 2'].astype('category')\n",
    "# data['Col 2'] = data['Col 2'].cat.codes\n",
    "\n",
    "# test_data = pd.read_csv('x_test.csv')\n",
    "# test_data['Col 2'] = test_data['Col 2'].astype('category')\n",
    "# test_data['Col 2'] = test_data['Col 2'].cat.codes\n",
    "\n",
    "y = data['y']\n",
    "X = data.drop(['y','id'], axis=1)\n",
    "X_test = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "selector = SelectKBest(f_regression, k=4)\n",
    "selector.fit(X, y)\n",
    "X = X[X.columns[selector.get_support(indices=True)]]\n",
    "X_test = X_test[X_test.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "# Feature Selection with polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X = poly.fit_transform(X)\n",
    "X_test = poly.fit_transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "models = [\n",
    "    {'name': 'Lasso Regression', 'model': Lasso(), 'params': {'alpha': [0.1, 0.5, 1, 2, 5, 10, 20, 50, 100]}},\n",
    "    {'name': 'Random Forest', 'model': RandomForestRegressor(), 'params': {'bootstrap': [True], 'max_depth': [4], 'max_features': ['auto'], 'min_samples_leaf': [5], 'min_samples_split': [3], 'n_estimators': [311],}},\n",
    "    {'name': 'Gradient Boosting Regressor', 'model': GradientBoostingRegressor(), 'params': {'n_estimators':[350], 'min_samples_split':[15], 'max_depth':[1], 'learning_rate':[0.01],}},\n",
    "]\n",
    "\n",
    "for modell in models:\n",
    "    # Cross Validation with params\n",
    "    grid = GridSearchCV(modell['model'], modell['params'], cv=5, scoring='r2')\n",
    "    grid.fit(x_train, y_train)\n",
    "    \n",
    "    # Now train with best params\n",
    "    model = modell['model'].set_params(**grid.best_params_)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Evaluate\n",
    "    print(model)\n",
    "    print('R2 Score: ', r2_score(y_test, y_pred))\n",
    "    print('MSE: ', mean_squared_error(y_test, y_pred))\n",
    "    print('CV Score R2: ', cross_val_score(model, x_train, y_train, cv=5, scoring='r2').mean())\n",
    "    print('CV Score MSE: ', abs(cross_val_score(model, x_train, y_train, cv=5, scoring='neg_mean_squared_error').mean()))\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "    # # Predict on test data\n",
    "    # y_pred = model.predict(x_test)\n",
    "    # test_data['y'] = y_pred\n",
    "    # test_data[['id', 'y']].to_csv(f'{modell[\"name\"]}.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Regressor\n",
      "R2 Score:  0.48926304582296964\n",
      "MSE:  3298.709833626974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "y = data['y']\n",
    "X = data.drop(['y','id'], axis=1)\n",
    "X_test = test_data.drop(['id'], axis=1)\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "selector = SelectKBest(f_regression, k=4)\n",
    "selector.fit(X, y)\n",
    "X = X[X.columns[selector.get_support(indices=True)]]\n",
    "X_test = X_test[X_test.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "# Feature Selection with polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X = poly.fit_transform(X)\n",
    "X_test = poly.fit_transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define the individual regressor models\n",
    "lasso_model = Lasso(alpha=0.1)  # You can adjust alpha as needed\n",
    "random_forest_model = RandomForestRegressor(bootstrap=True, max_depth=4, max_features='auto', min_samples_leaf=5, min_samples_split=3, n_estimators=311)\n",
    "gradient_boosting_model = GradientBoostingRegressor(n_estimators=350, min_samples_split=15, max_depth=1, learning_rate=0.01)\n",
    "\n",
    "# Create the Voting Regressor model\n",
    "voting_regressor = VotingRegressor(estimators=[\n",
    "    ('Lasso', lasso_model),\n",
    "    ('RandomForest', random_forest_model),\n",
    "    ('GradientBoosting', gradient_boosting_model)\n",
    "])\n",
    "\n",
    "# Fit the Voting Regressor model\n",
    "voting_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = voting_regressor.predict(x_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Voting Regressor\")\n",
    "print('R2 Score: ', r2_score(y_test, y_pred))\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred = voting_regressor.predict(X_test)\n",
    "test_data['y'] = y_pred\n",
    "test_data[['id', 'y']].to_csv('Brian_Voting_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-94ae0c019d69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_regression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('x_test.csv')\n",
    "# test_data['Col 2'] = test_data['Col 2'].astype('category')\n",
    "# test_data['Col 2'] = test_data['Col 2'].cat.codes\n",
    "\n",
    "X_test = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "selector = SelectKBest(f_regression, k=4)\n",
    "selector.fit(X, y)\n",
    "X = X[X.columns[selector.get_support(indices=True)]]\n",
    "X_test = X_test[X_test.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "# Feature Selection with polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X = poly.fit_transform(X)\n",
    "X_test = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00000000e+00, -6.22521820e-02, -7.45280244e-02,  4.46044580e-03,\n",
       "       -3.58167281e-02,  3.87533416e-03,  4.63953214e-03, -2.77672484e-04,\n",
       "        2.22966948e-03,  5.55442643e-03, -3.32428214e-04,  2.66934999e-03,\n",
       "        1.98955767e-05, -1.59758574e-04,  1.28283801e-03])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)\n",
    "3278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cde9e06954608812f36a56132da0251c351f6ad8984d203ba87e4f78021e1e3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
